{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dc021e3416004de89149d3d8aa9e2cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc82110d6b984c6f84dd39e1d9e2fbd8",
              "IPY_MODEL_b30f373a22fb4efaa62c303ef0ec053f",
              "IPY_MODEL_7c1ee1c9c00542c7b1d1812ffd0be614"
            ],
            "layout": "IPY_MODEL_449a8f4858eb44dfb4e4b500a8c8c9c3"
          }
        },
        "bc82110d6b984c6f84dd39e1d9e2fbd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aa3ee2de1ed48f2b49436520e2bb339",
            "placeholder": "​",
            "style": "IPY_MODEL_ee9013526dbc4255a3f9449d5b53a3a2",
            "value": "Map: 100%"
          }
        },
        "b30f373a22fb4efaa62c303ef0ec053f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b266430290e34f1ebf2b21f93c0bfa1e",
            "max": 1043,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_651b640cbc0f4a3f997fbf59524bf08f",
            "value": 1043
          }
        },
        "7c1ee1c9c00542c7b1d1812ffd0be614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8678a42e48b94bc6b7259f0e557dd359",
            "placeholder": "​",
            "style": "IPY_MODEL_bdcd30865f3f49a89df7234ab89af003",
            "value": " 1043/1043 [00:00&lt;00:00, 6934.54 examples/s]"
          }
        },
        "449a8f4858eb44dfb4e4b500a8c8c9c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aa3ee2de1ed48f2b49436520e2bb339": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee9013526dbc4255a3f9449d5b53a3a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b266430290e34f1ebf2b21f93c0bfa1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "651b640cbc0f4a3f997fbf59524bf08f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8678a42e48b94bc6b7259f0e557dd359": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdcd30865f3f49a89df7234ab89af003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anaghasid/GenPruning/blob/main/T5_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet datasets\n",
        "!pip install --quiet accelerate\n",
        "!pip install --quiet bitsandbytes"
      ],
      "metadata": {
        "id": "1ryZjCbQeunj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C57vO77Ra9B1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
        "from datasets import load_dataset, load_metric\n",
        "import accelerate\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pretrained model and tokenizer\n",
        "model_name = \"google-t5/t5-small\"\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True, # enable 4-bit quantization\n",
        "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
        "    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
        "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, trust_remote_code=True,\n",
        "                                                           device_map=device)\n",
        "                                                          #  quantization_config=quantization_config)  # Assuming binary classification task\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDQmZmXobbib",
        "outputId": "2ffedb41-4954-46f7-cfe0-f0b4ceacda74",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GLUE dataset (SST-2 as an example)\n",
        "dataset = load_dataset(\"glue\", \"cola\", split='validation')\n",
        "print(\"Sample from the dataset:\", dataset[0])\n",
        "metric = load_metric(\"glue\", \"cola\")"
      ],
      "metadata": {
        "id": "ICphpP9abfxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efdba031-05fc-47c5-b039-4465d78e8266",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample from the dataset: {'sentence': 'The sailors rode the breeze clear of the rocks.', 'label': 1, 'idx': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wxb3ox8466Y5",
        "outputId": "ab4ebcbf-a3f1-40c2-a590-7dc7897e7d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf5FpCkqyaFs",
        "outputId": "4046ad8d-10d6-46a5-f7bb-a0a09a5e44f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence': ['The sailors rode the breeze clear of the rocks.',\n",
              "  'The weights made the rope stretch over the pulley.',\n",
              "  'The mechanical doll wriggled itself loose.',\n",
              "  'If you had eaten more, you would want less.',\n",
              "  'As you eat the most, you want the least.',\n",
              "  'The more you would want, the less you would eat.',\n",
              "  'I demand that the more John eat, the more he pays.',\n",
              "  'Mary listens to the Grateful Dead, she gets depressed.',\n",
              "  'The angrier Mary got, the more she looked at pictures.',\n",
              "  'The higher the stakes, the lower his expectations are.'],\n",
              " 'label': [1, 1, 1, 1, 0, 0, 0, 1, 1, 1],\n",
              " 'idx': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "def preprocess_function(examples):\n",
        "    tokenized_inputs = tokenizer(examples['sentence'], truncation=True, padding='max_length', max_length=128)\n",
        "    tokenized_inputs['label'] = examples['label']\n",
        "    return tokenized_inputs\n",
        "\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "np6xWRrqbjrN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "dc021e3416004de89149d3d8aa9e2cfd",
            "bc82110d6b984c6f84dd39e1d9e2fbd8",
            "b30f373a22fb4efaa62c303ef0ec053f",
            "7c1ee1c9c00542c7b1d1812ffd0be614",
            "449a8f4858eb44dfb4e4b500a8c8c9c3",
            "8aa3ee2de1ed48f2b49436520e2bb339",
            "ee9013526dbc4255a3f9449d5b53a3a2",
            "b266430290e34f1ebf2b21f93c0bfa1e",
            "651b640cbc0f4a3f997fbf59524bf08f",
            "8678a42e48b94bc6b7259f0e557dd359",
            "bdcd30865f3f49a89df7234ab89af003"
          ]
        },
        "outputId": "ef6f1445-aa89-4d99-ba99-806e41b7a994",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1043 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc021e3416004de89149d3d8aa9e2cfd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DataLoader to create batches\n",
        "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "dataloader = DataLoader(encoded_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "zgnpdil6vmUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "dataloader = DataLoader(encoded_dataset, batch_size=16)\n",
        "\n",
        "\n",
        "# Display a sample batch from the dataloader\n",
        "for batch in dataloader:\n",
        "    # print(batch['label'])\n",
        "    input_ids = batch['input_ids'].tolist()\n",
        "    attention_masks = batch['attention_mask'].tolist()\n",
        "    labels = batch['label'].tolist()\n",
        "\n",
        "    truncated_input_ids = [ids[:10] for ids in input_ids]  # Show only the first 10 tokens\n",
        "    truncated_attention_masks = [masks[:10] for masks in attention_masks]\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'Input IDs': truncated_input_ids,\n",
        "        'Attention Mask': truncated_attention_masks,\n",
        "        'Label': labels\n",
        "    })\n",
        "\n",
        "    print(df)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4KKHUgDNuc0",
        "outputId": "a2585049-e059-408a-c79a-e1a85edd2628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Input IDs                  Attention Mask  Label\n",
            "0      [37, 30899, 6102, 15, 8, 15825, 964, 13, 8, 12288]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1\n",
            "1        [37, 1293, 7, 263, 8, 13888, 6606, 147, 8, 3197]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1\n",
            "2   [37, 8168, 14295, 3, 210, 23983, 1361, 1402, 6044, 5]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1\n",
            "3         [156, 25, 141, 16929, 72, 6, 25, 133, 241, 705]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1\n",
            "4               [282, 25, 3, 1544, 8, 167, 6, 25, 241, 8]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      0\n",
            "5              [37, 72, 25, 133, 241, 6, 8, 705, 25, 133]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      0\n",
            "6              [27, 2173, 24, 8, 72, 1079, 3, 1544, 6, 8]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      0\n",
            "7        [3790, 3011, 7, 12, 8, 350, 2206, 1329, 9651, 6]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1\n",
            "8           [37, 3, 1468, 6711, 3790, 530, 6, 8, 72, 255]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1\n",
            "9           [37, 1146, 8, 8474, 7, 6, 8, 1364, 112, 4454]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1\n",
            "10        [37, 72, 12264, 19, 3, 32, 115, 19864, 2936, 6]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1\n",
            "11     [1079, 47, 1995, 72, 3, 32, 115, 19864, 2936, 145]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1\n",
            "12             [37, 72, 151, 25, 428, 6061, 12, 6, 8, 72]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1\n",
            "13       [37, 72, 405, 3259, 7269, 6, 8, 72, 10445, 5591]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      0\n",
            "14         [37, 72, 1933, 13, 376, 24, 2385, 16, 8, 1506]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1\n",
            "15      [2181, 20596, 1330, 12, 582, 72, 17261, 6, 38, 3]  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir=\"./logs\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO0bCNqAblps",
        "outputId": "dcd5a8ca-2e3b-4d09-c7cb-9c1f9dde6c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Modify model based on chromosome\n",
        "def modify_model(model, chromosome):\n",
        "    num_heads = model.config.num_heads\n",
        "    num_blocks = model.config.num_layers\n",
        "    print(num_blocks)\n",
        "    # num_ffn = model.config.num_hidden_layers\n",
        "\n",
        "    # Disable attention heads\n",
        "    heads_to_prune = defaultdict(list)\n",
        "    for i, gene in enumerate(chromosome):\n",
        "        if gene == 0:\n",
        "          block_num = i//num_heads\n",
        "          head_num = i%num_heads\n",
        "          heads_to_prune[block_num].append(head_num)\n",
        "\n",
        "    print(heads_to_prune)\n",
        "    if heads_to_prune:\n",
        "\n",
        "        for block_num in heads_to_prune:\n",
        "            print(\"Pruning\")\n",
        "            model.transformer.encoder.block[block_num].layer[0].SelfAttention.prune_heads(heads_to_prune[block_num])\n",
        "            # if(bloack_num!=0):\n",
        "            model.transformer.decoder.block[block_num].layer[0].SelfAttention.prune_heads(heads_to_prune[block_num])\n",
        "            # model.transformer.decoder.block[block_num].layer[1].EncDecAttention.prune_heads(heads_to_prune[block_num])\n",
        "\n",
        "            print(block_num, model.transformer.encoder.block[block_num].layer[0].SelfAttention.n_heads)\n",
        "            # model.prune_heads(heads_to_prune)\n",
        "\n",
        "            # model.transformer.encoder.block[block_num].layer[0].SelfAttention.q = prune_linear_layer(model.transformer.encoder.block[block_num].layer[0].SelfAttention.q, index)\n",
        "            # model.transformer.encoder.block[block_num].layer[0].SelfAttention.k = prune_linear_layer(model.transformer.encoder.block[block_num].layer[0].SelfAttention.k, index)\n",
        "            # model.transformer.encoder.block[block_num].layer[0].SelfAttention.v = prune_linear_layer(model.transformer.encoder.block[block_num].layer[0].SelfAttention.v, index)\n",
        "            # model.transformer.encoder.block[block_num].layer[0].SelfAttention.o = prune_linear_layer(model.transformer.encoder.block[block_num].layer[0].SelfAttention.o, index, dim=1)"
      ],
      "metadata": {
        "id": "ZHxSB1I0bn-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_fitness(chromosome, model, encoded_dataset, metric):\n",
        "    # Modify the model according to the chromosome\n",
        "    # if(any(chromosome)):\n",
        "    modify_model(model, chromosome)\n",
        "\n",
        "    start_time = time.time()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Get predictions\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            labels.extend(batch['label'].cpu().numpy())\n",
        "\n",
        "    # Print a few predictions and their corresponding labels\n",
        "    for i in range(10):\n",
        "        print(f\"Prediction: {predictions[i]}, Label: {labels[i]}\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    # eval_result = trainer.evaluate()\n",
        "    # fitness = eval_result['eval_accuracy']\n",
        "    fitness = accuracy_score(labels, predictions)\n",
        "    print(f\"Validation Accuracy: {fitness:.4f}\")\n",
        "    end_time = time.time()\n",
        "    print(\"Execution time = \",end_time-start_time)\n",
        "\n",
        "    return fitness"
      ],
      "metadata": {
        "id": "BvaktIYnbqyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# without modifying the model\n",
        "evaluate_fitness([False], model, encoded_dataset, metric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2FwHJjs1ClV",
        "outputId": "3e2b0da2-4d15-443c-a5b4-ea91644953d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [0]})\n",
            "Prediction: 1, Label: 1\n",
            "Prediction: 1, Label: 1\n",
            "Prediction: 1, Label: 1\n",
            "Prediction: 1, Label: 1\n",
            "Prediction: 1, Label: 0\n",
            "Prediction: 1, Label: 0\n",
            "Prediction: 1, Label: 0\n",
            "Prediction: 1, Label: 1\n",
            "Prediction: 1, Label: 1\n",
            "Prediction: 1, Label: 1\n",
            "Validation Accuracy: 0.6913\n",
            "Execution time =  264.4926497936249\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6912751677852349"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "FLFMXsKwJDND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      eval_dataset=encoded_dataset,\n",
        "      compute_metrics=lambda p: metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
        "  )\n",
        "\n",
        "# Evaluate the model\n",
        "eval_result = trainer.evaluate()\n",
        "fitness = eval_result['eval_accuracy']\n",
        "print(fitness)\n",
        "'''\n"
      ],
      "metadata": {
        "id": "X0-lDneZNx7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dir(model):\n",
        "  if not i.startswith(\"_\"):\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "C-14kwFcnXvS",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1153e17-1013-4b28-9aea-28b42ff8cd71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T_destination\n",
            "active_adapter\n",
            "active_adapters\n",
            "add_adapter\n",
            "add_memory_hooks\n",
            "add_model_tags\n",
            "add_module\n",
            "apply\n",
            "base_model\n",
            "base_model_prefix\n",
            "bfloat16\n",
            "buffers\n",
            "call_super_init\n",
            "can_generate\n",
            "children\n",
            "classification_head\n",
            "compile\n",
            "compute_transition_scores\n",
            "config\n",
            "config_class\n",
            "contrastive_search\n",
            "cpu\n",
            "create_extended_attention_mask_for_decoder\n",
            "cuda\n",
            "dequantize\n",
            "device\n",
            "disable_adapters\n",
            "disable_input_require_grads\n",
            "double\n",
            "dtype\n",
            "dummy_inputs\n",
            "dump_patches\n",
            "enable_adapters\n",
            "enable_input_require_grads\n",
            "estimate_tokens\n",
            "eval\n",
            "extra_repr\n",
            "float\n",
            "floating_point_ops\n",
            "forward\n",
            "framework\n",
            "from_pretrained\n",
            "generate\n",
            "generation_config\n",
            "get_adapter_state_dict\n",
            "get_buffer\n",
            "get_extended_attention_mask\n",
            "get_extra_state\n",
            "get_head_mask\n",
            "get_input_embeddings\n",
            "get_memory_footprint\n",
            "get_output_embeddings\n",
            "get_parameter\n",
            "get_position_embeddings\n",
            "get_submodule\n",
            "gradient_checkpointing_disable\n",
            "gradient_checkpointing_enable\n",
            "half\n",
            "heal_tokens\n",
            "hf_device_map\n",
            "init_weights\n",
            "invert_attention_mask\n",
            "ipu\n",
            "is_gradient_checkpointing\n",
            "is_parallelizable\n",
            "load_adapter\n",
            "load_state_dict\n",
            "load_tf_weights\n",
            "main_input_name\n",
            "model_parallel\n",
            "model_tags\n",
            "modules\n",
            "name_or_path\n",
            "named_buffers\n",
            "named_children\n",
            "named_modules\n",
            "named_parameters\n",
            "num_parameters\n",
            "parameters\n",
            "post_init\n",
            "prepare_inputs_for_generation\n",
            "prune_heads\n",
            "push_to_hub\n",
            "register_backward_hook\n",
            "register_buffer\n",
            "register_for_auto_class\n",
            "register_forward_hook\n",
            "register_forward_pre_hook\n",
            "register_full_backward_hook\n",
            "register_full_backward_pre_hook\n",
            "register_load_state_dict_post_hook\n",
            "register_module\n",
            "register_parameter\n",
            "register_state_dict_pre_hook\n",
            "requires_grad_\n",
            "reset_memory_hooks_state\n",
            "resize_position_embeddings\n",
            "resize_token_embeddings\n",
            "retrieve_modules_from_names\n",
            "reverse_bettertransformer\n",
            "save_pretrained\n",
            "set_adapter\n",
            "set_extra_state\n",
            "set_input_embeddings\n",
            "share_memory\n",
            "state_dict\n",
            "supports_gradient_checkpointing\n",
            "tie_weights\n",
            "to\n",
            "to_bettertransformer\n",
            "to_empty\n",
            "train\n",
            "training\n",
            "transformer\n",
            "type\n",
            "warn_if_padding_and_no_attention_mask\n",
            "warnings_issued\n",
            "xpu\n",
            "zero_grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def create_random_binary_list(length, percentage_of_zeros):\n",
        "    # Calculate the number of zeros and ones based on the percentage\n",
        "    num_zeros = int(length * (percentage_of_zeros / 100))\n",
        "    num_ones = length - num_zeros\n",
        "\n",
        "    # Create the list with the required number of 0s and 1s\n",
        "    binary_list = [0] * num_zeros + [1] * num_ones\n",
        "\n",
        "    # Shuffle the list to randomize the order\n",
        "    random.shuffle(binary_list)\n",
        "\n",
        "    return binary_list"
      ],
      "metadata": {
        "id": "4mHUj59O30pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_chromosome(num_heads, num_layers):\n",
        "    return create_random_binary_list(num_heads * num_layers, 30)"
      ],
      "metadata": {
        "id": "N5wxFL7LysN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config.is_decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEoH-2Li5O07",
        "outputId": "351b49a2-e097-46ae-8d05-4f7db1e52b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chromosome = np.random.randint(2, size=(model.config.num_attention_heads + model.config.num_hidden_layers))\n",
        "model  = AutoModelForSequenceClassification.from_pretrained(model_name, trust_remote_code=True, device_map=device)\n",
        "chromosome = initialize_chromosome(model.config.num_heads, model.config.num_layers)\n",
        "print(\"Chromosome:\", chromosome)\n",
        "fitness = evaluate_fitness(chromosome, model, encoded_dataset, metric)\n",
        "print(\"Fitness score (accuracy):\", fitness)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCNP5DfCbTES",
        "outputId": "a6e249f0-a277-4f93-9a02-c3280a9abbe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chromosome: [1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "6\n",
            "defaultdict(<class 'list'>, {0: [2, 5, 7], 1: [4, 5, 7], 2: [2, 3, 4, 5, 6], 3: [4, 5, 7]})\n",
            "Pruning\n",
            "0 5\n",
            "Pruning\n",
            "1 5\n",
            "Pruning\n",
            "2 3\n",
            "Pruning\n",
            "3 5\n",
            "Prediction: 0, Label: 1\n",
            "Prediction: 0, Label: 1\n",
            "Prediction: 0, Label: 1\n",
            "Prediction: 0, Label: 1\n",
            "Prediction: 0, Label: 0\n",
            "Prediction: 0, Label: 0\n",
            "Prediction: 0, Label: 0\n",
            "Prediction: 0, Label: 1\n",
            "Prediction: 0, Label: 1\n",
            "Prediction: 0, Label: 1\n",
            "Validation Accuracy: 0.3087\n",
            "Execution time =  236.12051367759705\n",
            "Fitness score (accuracy): 0.3087248322147651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chromosome = np.random.randint(2, size=(model.config.num_attention_heads + model.config.num_hidden_layers))\n",
        "model  = AutoModelForSequenceClassification.from_pretrained(model_name, trust_remote_code=True, device_map=device)\n",
        "chromosome = initialize_chromosome(model.config.num_heads, model.config.num_layers)    # only attn heads for now\n",
        "print(\"Chromosome:\", chromosome)\n",
        "fitness = evaluate_fitness(chromosome, model, encoded_dataset, metric)\n",
        "print(\"Fitness score (accuracy):\", fitness)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbovP2i7zugb",
        "outputId": "143db3e7-087a-41c3-ac4d-cd0e48496775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chromosome: [0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 0 1 1 0 1 0 1 0 1 0]\n",
            "6\n",
            "defaultdict(<class 'list'>, {0: [0, 2, 3, 6], 1: [0, 1, 2, 4, 5, 6, 7], 2: [0, 1, 2, 4, 5, 6, 7], 3: [0, 3, 6], 4: [3, 5, 6], 5: [1, 3, 5, 7]})\n",
            "Pruning\n",
            "0 4\n",
            "Pruning\n",
            "1 1\n",
            "Pruning\n",
            "2 1\n",
            "Pruning\n",
            "3 5\n",
            "Pruning\n",
            "4 5\n",
            "Pruning\n",
            "5 4\n",
            "Prediction: 1, Label: 1\n",
            "Prediction: 1, Label: 1\n",
            "Prediction: 1, Label: 1\n",
            "Prediction: 0, Label: 1\n",
            "Prediction: 0, Label: 0\n",
            "Prediction: 1, Label: 0\n",
            "Prediction: 0, Label: 0\n",
            "Prediction: 1, Label: 1\n",
            "Prediction: 1, Label: 1\n",
            "Prediction: 1, Label: 1\n",
            "Validation Accuracy: 0.6769\n",
            "Execution time =  243.05039262771606\n",
            "Fitness score (accuracy): 0.6768935762224353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_size(model):\n",
        "  total_size_in_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "\n",
        "  # Convert to megabytes (MB)\n",
        "  total_size_in_megabytes = total_size_in_bytes / (1024 ** 2)\n",
        "\n",
        "  print(f\"Model size: {total_size_in_megabytes:.2f} MB\")"
      ],
      "metadata": {
        "id": "I4_Q6d-62AhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m  = AutoModelForSequenceClassification.from_pretrained(model_name, trust_remote_code=True, device_map=device)\n",
        "print(find_size(m), find_size(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u43nPciC2aIm",
        "outputId": "bc0a0096-99c4-4926-92b7-f61eb74ab34d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 231.82 MB\n",
            "Model size: 217.82 MB\n",
            "None None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chromosome = initialize_chromosome(model.config.num_heads, model.config.num_layers)\n",
        "fitness = evaluate_fitness(chromosome, model, encoded_dataset, metric)\n",
        "print(\"Fitness score (accuracy):\", fitness)"
      ],
      "metadata": {
        "id": "DQxcqC9z2oLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b096d92c-54ef-4298-8f36-5051b3b7136a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 0, Label: 1\n",
            "Prediction: 0, Label: 1\n",
            "Prediction: 0, Label: 1\n",
            "Prediction: 0, Label: 1\n",
            "Prediction: 0, Label: 0\n",
            "Prediction: 0, Label: 0\n",
            "Prediction: 0, Label: 0\n",
            "Prediction: 0, Label: 1\n",
            "Prediction: 0, Label: 1\n",
            "Prediction: 0, Label: 1\n",
            "Validation Accuracy: 0.3087\n",
            "Execution time =  236.12051367759705\n",
            "Fitness score (accuracy): 0.3087248322147651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Selection (Tournament Selection)\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(population_size):\n",
        "        tournament = np.random.choice(population_size, 2)\n",
        "        winner = tournament[np.argmax(fitness_scores[tournament])]\n",
        "        parents.append(population[winner])\n",
        "    return np.array(parents)"
      ],
      "metadata": {
        "id": "g6JzUlKUk4Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crossover (Single-point crossover)\n",
        "def crossover(parent1, parent2):\n",
        "    if np.random.rand() < crossover_rate:\n",
        "        point = np.random.randint(1, len(parent1) - 1)\n",
        "        child1 = np.concatenate([parent1[:point], parent2[point:]])\n",
        "        child2 = np.concatenate([parent2[:point], parent1[point:]])\n",
        "    else:\n",
        "        child1, child2 = parent1, parent2\n",
        "    return child1, child2"
      ],
      "metadata": {
        "id": "0X9ruDzQk5ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mutation (Flip bit mutation)\n",
        "def mutate(chromosome):\n",
        "    for i in range(len(chromosome)):\n",
        "        if np.random.rand() < mutation_rate:\n",
        "            chromosome[i] = 1 - chromosome[i]\n",
        "    return chromosome"
      ],
      "metadata": {
        "id": "hPGjVtb2k_L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genetic_algorithm(model, num_generations, desired_sparsity):\n",
        "  population = initialize_population(population_size, len(model.config.attention_heads) + len(model.config.hidden_layers))\n",
        "  for generation in range(num_generations):\n",
        "      fitness_scores = np.array([evaluate_fitness(chrom, model) for chrom in population])\n",
        "      parents = select_parents(population, fitness_scores)\n",
        "      new_population = []\n",
        "      for i in range(0, population_size, 2):\n",
        "          parent1, parent2 = parents[i], parents[i + 1]\n",
        "          child1, child2 = crossover(parent1, parent2)\n",
        "          child1 = mutate(child1)\n",
        "          child2 = mutate(child2)\n",
        "          new_population.extend([child1, child2])\n",
        "      population = np.array(new_population)\n",
        "\n",
        "      # Check for desired sparsity level\n",
        "      sparsity_levels = np.mean(population == 0, axis=1)\n",
        "      if np.any(sparsity_levels >= desired_sparsity):\n",
        "          best_chromosome = population[np.argmax(sparsity_levels)]\n",
        "          break\n",
        "  return best_chromosome"
      ],
      "metadata": {
        "id": "Kajvda-3lD5n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}