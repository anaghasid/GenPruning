{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11384,"sourceType":"modelInstanceVersion","modelInstanceId":6216,"modelId":3301}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-06T16:06:20.260755Z","iopub.execute_input":"2024-11-06T16:06:20.261073Z","iopub.status.idle":"2024-11-06T16:06:21.371711Z","shell.execute_reply.started":"2024-11-06T16:06:20.261038Z","shell.execute_reply":"2024-11-06T16:06:21.370765Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/gemma/transformers/2b/2/model.safetensors.index.json\n/kaggle/input/gemma/transformers/2b/2/gemma-2b.gguf\n/kaggle/input/gemma/transformers/2b/2/config.json\n/kaggle/input/gemma/transformers/2b/2/model-00001-of-00002.safetensors\n/kaggle/input/gemma/transformers/2b/2/model-00002-of-00002.safetensors\n/kaggle/input/gemma/transformers/2b/2/tokenizer.json\n/kaggle/input/gemma/transformers/2b/2/tokenizer_config.json\n/kaggle/input/gemma/transformers/2b/2/special_tokens_map.json\n/kaggle/input/gemma/transformers/2b/2/.gitattributes\n/kaggle/input/gemma/transformers/2b/2/tokenizer.model\n/kaggle/input/gemma/transformers/2b/2/generation_config.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# !pip install bert-score\n# !pip install rouge-score\n# from nltk.translate.bleu_score import sentence_bleu\n# from rouge_score import rouge_scorer\nimport nltk\n# from transformers.modeling_utils import prune_linear_layer\nfrom collections import defaultdict\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom torch.utils.data.dataset import Dataset\nimport gc\nimport math\nimport time","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:06:21.375053Z","iopub.execute_input":"2024-11-06T16:06:21.375514Z","iopub.status.idle":"2024-11-06T16:06:27.571418Z","shell.execute_reply.started":"2024-11-06T16:06:21.375459Z","shell.execute_reply":"2024-11-06T16:06:27.570145Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class IndexDataset(Dataset):\n    def __init__(self, tensors):\n        self.tensors = tensors\n\n    def __getitem__(self, index):\n        return self.tensors[index]\n\n    def __len__(self):\n        return len(self.tensors)\n\ndef process_data(samples, tokenizer, seq_len, field_name):\n    test_ids = tokenizer(\"\\n\\n\".join(samples[field_name]), return_tensors='pt').input_ids[0]\n    test_ids_batch = []\n    nsamples = test_ids.numel() // seq_len\n\n    for i in range(nsamples):\n        batch = test_ids[(i * seq_len):((i + 1) * seq_len)]\n        test_ids_batch.append(batch)\n    test_ids_batch = torch.stack(test_ids_batch)\n    return IndexDataset(tensors=test_ids_batch)\n       \n\ndef get_loaders(tokenizer, seq_len=2048, batch_size = 8):\n    test_data = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n#     test_dataset = process_data(test_data, tokenizer, seq_len, 'text')\n    test_dataset = process_data(test_data[0:100], tokenizer, seq_len, 'text')\n\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    return test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T16:06:27.572639Z","iopub.execute_input":"2024-11-06T16:06:27.573200Z","iopub.status.idle":"2024-11-06T16:06:27.582430Z","shell.execute_reply.started":"2024-11-06T16:06:27.573163Z","shell.execute_reply":"2024-11-06T16:06:27.581445Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def make_model():\n    for _ in range(10):\n        # print(\"Emptying cache\")\n        torch.cuda.empty_cache()\n        gc.collect()\n    try:\n        del model\n        print(\"Deleted existing\")\n        for _ in range(10):\n            print(torch.cuda.empty_cache())\n            gc.collect()\n    except:\n        pass\n    time.sleep(6)\n    model = GemmaForCausalLM.from_pretrained(model_name)  # Use the appropriate model class\n    model.to(device)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:17:26.140669Z","iopub.execute_input":"2024-11-06T16:17:26.141569Z","iopub.status.idle":"2024-11-06T16:17:26.147398Z","shell.execute_reply.started":"2024-11-06T16:17:26.141528Z","shell.execute_reply":"2024-11-06T16:17:26.146397Z"},"trusted":true},"outputs":[],"execution_count":43},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModel, GemmaForCausalLM \nimport torch\nmodel_name = '/kaggle/input/gemma/transformers/2b/2'  \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = make_model()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:06:27.617816Z","iopub.execute_input":"2024-11-06T16:06:27.618206Z","iopub.status.idle":"2024-11-06T16:07:09.130666Z","shell.execute_reply.started":"2024-11-06T16:06:27.618165Z","shell.execute_reply":"2024-11-06T16:07:09.129619Z"},"trusted":true},"outputs":[{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d8f9afac9f04b84b84b79b08437b9ca"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"test_loader = get_loaders(tokenizer, seq_len=128, batch_size = 4)\ndef PPLMetric(model, tokenizer, seq_len=128, batch_size = 4, device=\"cuda\"):\n    metric = {}\n    metric = ppl_eval(model, test_loader, device)\n    print(metric)\n    return metric\n\n\ndef ppl_eval(model, test_loader, device):\n    '''model.eval()\n    total_log_prob = 0\n    total_count = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader):\n            inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True).to(model.device)\n            labels = inputs['input_ids']\n            \n            outputs = model(**inputs, labels=labels)\n            loss = outputs.loss\n            log_prob = -loss.item() * labels.numel()  # Negative log likelihood\n            total_log_prob += log_prob\n            total_count += labels.numel()\n    \n    avg_log_prob = total_log_prob / total_count\n    perplexity = math.exp(-avg_log_prob)\n    return perplexity'''\n\n    # nlls = []\n    # n_samples = 0\n    # with torch.no_grad():\n    #     for batch in tqdm(test_loader):\n    #         batch = batch.to(device)\n    #         # CHANGE THIS:\n    #         output = model(batch)\n    #         lm_logits = output.logits\n\n    #         shift_logits = lm_logits[:, :-1, :].contiguous()\n    #         shift_labels = batch[:, 1:].contiguous()\n\n    #         loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n    #         loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    #         nlls.append(loss)\n    #         del batch\n    #         for _ in range(10):\n    #             torch.cuda.empty_cache()\n    #             gc.collect()\n    # #print(torch.cat(nlls, dim=-1).mean())\n    # ppl = np.exp(torch.cat(nlls, dim=-1).mean().item())\n    # return ppl.item()\n    \n    model.eval()  # Ensure model is in evaluation mode\n    total_loss = 0\n    total_tokens = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader):\n            batch = batch.to(device)\n            \n            outputs = model(\n                input_ids=batch,\n                labels=batch  # Let the model handle the label shifting\n            )\n            \n            # Most models return loss directly\n            loss = outputs.loss\n            \n            # Accumulate total loss\n            total_loss += loss.item() * batch.size(1)  # Multiply by sequence length\n            total_tokens += batch.size(1)\n            \n            # Clean up memory\n            del batch, outputs\n            torch.cuda.empty_cache()\n    \n    # Calculate perplexity\n    avg_loss = total_loss / total_tokens\n    ppl = math.exp(avg_loss)\n    return ppl","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:08:25.088116Z","iopub.execute_input":"2024-11-06T16:08:25.089017Z","iopub.status.idle":"2024-11-06T16:08:31.248161Z","shell.execute_reply.started":"2024-11-06T16:08:25.088949Z","shell.execute_reply":"2024-11-06T16:08:31.247387Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import random\ndef create_random_binary_list(length, percentage_of_zeros):\n    num_zeros = int(length * percentage_of_zeros)\n    num_ones = length - num_zeros\n\n    # Create the list with the required number of 0s and 1s\n    binary_list = [0] * num_zeros + [1] * num_ones\n\n    # Shuffle the list to randomize the order\n    random.shuffle(binary_list)\n\n    return binary_list\n\ndef initialize_chromosome(num_genes):\n  # initialize chromosome with given sparsity percentage\n  return create_random_binary_list(num_genes, SPARSITY_RATE)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:07:20.382808Z","iopub.execute_input":"2024-11-06T16:07:20.383128Z","iopub.status.idle":"2024-11-06T16:07:20.389140Z","shell.execute_reply.started":"2024-11-06T16:07:20.383095Z","shell.execute_reply":"2024-11-06T16:07:20.388148Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def modify_model(model, chromosome):\n    num_heads = num_attention_heads\n    num_blocks = num_hidden_layers\n    # num_ffn = model.config.num_hidden_layers\n\n    # Disable attention heads\n    heads_to_prune = defaultdict(list)\n    for i, gene in enumerate(chromosome):\n        if gene == 0:\n          block_num = i//num_heads\n          head_num = i%num_heads\n          heads_to_prune[block_num].append(head_num)\n\n    head_dim = model.config.head_dim\n    if heads_to_prune:\n        print(\"Pruning heads in model\")\n        with torch.no_grad():\n            for block in range(block_num): \n                for head in heads_to_prune[block]:\n                        # Zero-out the corresponding rows in the q_proj, k_proj, and v_proj\n                        start_index = head * head_dim\n                        end_index = (head + 1) * head_dim\n                        model.model.layers[block].self_attn.q_proj.weight[start_index:end_index, :] = 0\n                        model.model.layers[block].self_attn.k_proj.weight[start_index:end_index, :] = 0\n                        model.model.layers[block].self_attn.v_proj.weight[start_index:end_index, :] = 0\n\n    #                     model.model.layers[block_num].self_attn.q_proj.weight = model.model.layers[block_num].self_attn.q_proj.weight.to(torch.int8)\n    #                     model.model.layers[block_num].self_attn.k_proj.weight = model.model.layers[block_num].self_attn.q_proj.weight.to(torch.int8)\n    #                     model.model.layers[block_num].self_attn.c_proj.weight = model.model.layers[block_num].self_attn.q_proj.weight.to(torch.int8)\n\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:24:19.377797Z","iopub.execute_input":"2024-11-06T16:24:19.378687Z","iopub.status.idle":"2024-11-06T16:24:19.387675Z","shell.execute_reply.started":"2024-11-06T16:24:19.378636Z","shell.execute_reply":"2024-11-06T16:24:19.386649Z"},"trusted":true},"outputs":[],"execution_count":50},{"cell_type":"code","source":"def evaluate_fitness(chromosome):\n    model = make_model()\n    model = modify_model(model,chromosome)\n    metric = PPLMetric(model, tokenizer)\n    print(\"Chromosome:\", chromosome, \"\\nFitness:\", metric)\n    del model\n    return (-1)*metric","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:26:56.293748Z","iopub.execute_input":"2024-11-06T16:26:56.294180Z","iopub.status.idle":"2024-11-06T16:26:56.300893Z","shell.execute_reply.started":"2024-11-06T16:26:56.294143Z","shell.execute_reply":"2024-11-06T16:26:56.300024Z"},"trusted":true},"outputs":[],"execution_count":52},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:07:20.422565Z","iopub.execute_input":"2024-11-06T16:07:20.423437Z","iopub.status.idle":"2024-11-06T16:07:20.429384Z","shell.execute_reply.started":"2024-11-06T16:07:20.423403Z","shell.execute_reply":"2024-11-06T16:07:20.428595Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def find_size(model):\n  total_size_in_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n  total_size_in_megabytes = total_size_in_bytes / (1024 ** 2)\n  print(f\"Model size: {total_size_in_megabytes:.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:07:20.430344Z","iopub.execute_input":"2024-11-06T16:07:20.430605Z","iopub.status.idle":"2024-11-06T16:07:20.439456Z","shell.execute_reply.started":"2024-11-06T16:07:20.430577Z","shell.execute_reply":"2024-11-06T16:07:20.438747Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# del model\nmodel = make_model()\nmetric = PPLMetric(model, tokenizer)\nfind_size(model)\nnum_attention_heads = model.config.num_attention_heads\nnum_hidden_layers = model.config.num_hidden_layers","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:10:21.865404Z","iopub.execute_input":"2024-11-06T16:10:21.865795Z","iopub.status.idle":"2024-11-06T16:10:37.025720Z","shell.execute_reply.started":"2024-11-06T16:10:21.865759Z","shell.execute_reply":"2024-11-06T16:10:37.024735Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Emptying cache\nEmptying cache\nEmptying cache\nEmptying cache\nEmptying cache\nEmptying cache\nEmptying cache\nEmptying cache\nEmptying cache\nEmptying cache\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0515e821e024de3bdeab6204849dfc3"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 11/11 [00:05<00:00,  2.15it/s]","output_type":"stream"},{"name":"stdout","text":"741.6232902902235\nModel size: 9560.29 MB\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"chrom = initialize_chromosome(num_attention_heads*num_hidden_layers)\n# model = modify_model(model, chrom)\n# find_size(model)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:11:05.251015Z","iopub.execute_input":"2024-11-06T16:11:05.251398Z","iopub.status.idle":"2024-11-06T16:11:05.255858Z","shell.execute_reply.started":"2024-11-06T16:11:05.251363Z","shell.execute_reply":"2024-11-06T16:11:05.254921Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"POPN_SIZE = 8\nSPARSITY_RATE = 0.3\ncrossover_rate = 0.7\nmutation_rate = 0.08\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:11:12.115395Z","iopub.execute_input":"2024-11-06T16:11:12.116117Z","iopub.status.idle":"2024-11-06T16:11:12.120296Z","shell.execute_reply.started":"2024-11-06T16:11:12.116076Z","shell.execute_reply":"2024-11-06T16:11:12.119217Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def initialize_population(chromosome_length):\n  # initialize random population\n  population = []\n  for _ in range(POPN_SIZE):\n    chromosome = initialize_chromosome(chromosome_length)\n    population.append(chromosome)\n  return population","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:11:12.124614Z","iopub.execute_input":"2024-11-06T16:11:12.125546Z","iopub.status.idle":"2024-11-06T16:11:12.130935Z","shell.execute_reply.started":"2024-11-06T16:11:12.125512Z","shell.execute_reply":"2024-11-06T16:11:12.130003Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def select_parents(population, fitness_scores, num_parents):\n    # Normalize fitness scores to create a probability distribution\n    total_fitness = np.sum(fitness_scores)\n    probabilities = fitness_scores / total_fitness\n\n    # Select parents based on their fitness proportion (roulette wheel selection)\n    # selected_parents = np.random.choice(population, size=num_parents, p=probabilities, replace=True)\n    selected_parents = random.choices(population, weights=probabilities, k=num_parents)\n\n\n    return np.array(selected_parents)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:11:12.132594Z","iopub.execute_input":"2024-11-06T16:11:12.132889Z","iopub.status.idle":"2024-11-06T16:11:12.143332Z","shell.execute_reply.started":"2024-11-06T16:11:12.132857Z","shell.execute_reply":"2024-11-06T16:11:12.142513Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Crossover (Single-point crossover)\ndef crossover(parent1, parent2):\n    if np.random.rand() < crossover_rate:\n        point = np.random.randint(1, len(parent1) - 1)\n        child1 = np.concatenate([parent1[:point], parent2[point:]])\n        child2 = np.concatenate([parent2[:point], parent1[point:]])\n    else:\n        child1, child2 = parent1, parent2\n    return child1, child2","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:11:12.144367Z","iopub.execute_input":"2024-11-06T16:11:12.144680Z","iopub.status.idle":"2024-11-06T16:11:12.154055Z","shell.execute_reply.started":"2024-11-06T16:11:12.144642Z","shell.execute_reply":"2024-11-06T16:11:12.153258Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Mutation (Flip bit mutation)\ndef mutate(chromosome):\n    for i in range(len(chromosome)):\n        if np.random.rand() < mutation_rate:\n            chromosome[i] = 1 - chromosome[i]\n\n    target_ones = int(len(chromosome) * (1-SPARSITY_RATE))\n\n    for c in range(num_attention_heads-1, len(chromosome), num_attention_heads):\n      # this part to ensure that each layer has at least one attention head\n      start = c-num_attention_heads-1\n      enc_part = chromosome[start:c]\n      num_ones = np.sum(enc_part)  # Count the number of 1s in the chromosome\n      if num_ones==0:\n        chromosome[start] = 1\n\n    for i in range(len(chromosome)):\n        if np.random.rand() < mutation_rate:\n            if chromosome[i] == 1 and num_ones > target_ones:\n                chromosome[i] = 0  # Flip 1 to 0 only if there are too many 1s\n                num_ones -= 1\n            elif chromosome[i] == 0 and num_ones < target_ones:\n                chromosome[i] = 1  # Flip 0 to 1 only if there are too few 1s\n                num_ones += 1\n    return chromosome\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:38:53.207592Z","iopub.execute_input":"2024-11-06T16:38:53.208229Z","iopub.status.idle":"2024-11-06T16:38:53.216434Z","shell.execute_reply.started":"2024-11-06T16:38:53.208187Z","shell.execute_reply":"2024-11-06T16:38:53.215449Z"},"trusted":true},"outputs":[],"execution_count":58},{"cell_type":"code","source":"def elitism_and_selection(population, fitness_scores, num_elites, num_parents):\n    # Elitism: Keep the top num_elites individuals\n    elite_indices = np.argsort(fitness_scores)[-num_elites:]  # Get indices of top individuals\n    elites = [population[i] for i in elite_indices]\n\n    # Perform roulette wheel selection for the rest of the parents\n    remaining_population = np.delete(population, elite_indices, axis=0)\n    remaining_fitness_scores = np.delete(fitness_scores, elite_indices)\n\n    num_to_select = num_parents - num_elites\n    selected_parents = select_parents(remaining_population, remaining_fitness_scores, num_to_select)\n\n    # Combine elites and selected parents\n    next_generation = np.vstack((elites, selected_parents))\n\n    return next_generation","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:11:12.165937Z","iopub.execute_input":"2024-11-06T16:11:12.166308Z","iopub.status.idle":"2024-11-06T16:11:12.178339Z","shell.execute_reply.started":"2024-11-06T16:11:12.166277Z","shell.execute_reply":"2024-11-06T16:11:12.177630Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef genetic_algorithm(num_generations, desired_sparsity):\n  population = initialize_population(num_attention_heads*num_hidden_layers)  # Initialize the population\n  accuracy_per_generation = []  # List to store highest accuracy values for each generation\n  for generation in range(num_generations):\n      fitness_scores = np.array([evaluate_fitness(chrom) for chrom in population])\n      best_chromosome = population[np.argmax(fitness_scores)]\n      print(\"new fitness scores:\", fitness_scores)\n      print(f\"best chromosome in generation {generation} is {best_chromosome} with accuracy {fitness_scores[np.argmax(fitness_scores)]}\")\n      accuracy_per_generation.append(fitness_scores[np.argmax(fitness_scores)])\n      parents = elitism_and_selection(population, fitness_scores, 4, POPN_SIZE)\n      # parents = select_parents(population, fitness_scores, POPN_SIZE)\n      new_population = []\n      for i in range(0, POPN_SIZE, 2):\n          parent1, parent2 = parents[i], parents[i + 1]\n          child1, child2 = crossover(parent1, parent2)\n          child1 = mutate(child1)\n          child2 = mutate(child2)\n          new_population.extend([child1, child2])\n      population = np.array(new_population)\n  generations = list(range(1, len(accuracy_per_generation) + 1))\n\n\n  return best_chromosome","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:35:30.966537Z","iopub.execute_input":"2024-11-06T16:35:30.967274Z","iopub.status.idle":"2024-11-06T16:35:30.975658Z","shell.execute_reply.started":"2024-11-06T16:35:30.967235Z","shell.execute_reply":"2024-11-06T16:35:30.974692Z"},"trusted":true},"outputs":[],"execution_count":56},{"cell_type":"code","source":"genetic_algorithm(2, SPARSITY_RATE)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:35:35.722326Z","iopub.execute_input":"2024-11-06T16:35:35.722708Z","iopub.status.idle":"2024-11-06T16:38:08.243423Z","shell.execute_reply.started":"2024-11-06T16:35:35.722665Z","shell.execute_reply":"2024-11-06T16:38:08.234794Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c92d86b049c54adf82bcd750e9fb582e"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:04<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"681.567115914516\nChromosome: [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1] \nFitness: 681.567115914516\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4097130bcee94e949145f4e4cefba48b"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:04<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"7185.827098166401\nChromosome: [1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1] \nFitness: 7185.827098166401\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c2af95f6d3b4d25a8b8017bcfa16419"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:04<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"3.019709328804503e+50\nChromosome: [0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1] \nFitness: 3.019709328804503e+50\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d3c2fab07ab48a2a60ca3b62e812802"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:04<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"2112.873610437727\nChromosome: [1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0] \nFitness: 2112.873610437727\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8a1e32029594e67a6206288f9f516e1"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:04<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"1.2917770633004654e+51\nChromosome: [0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \nFitness: 1.2917770633004654e+51\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43a95e6a757c4b0389fad8042a783eba"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:04<00:00,  2.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"961.3582048234236\nChromosome: [1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1] \nFitness: 961.3582048234236\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b82ac2d5c4cb477d8402a9f1c5c0ee72"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:04<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"1.7072522510152955e+48\nChromosome: [0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1] \nFitness: 1.7072522510152955e+48\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4130e5980dad4c5fbb4e950681c783d0"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:04<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"1.3130824248665481e+45\nChromosome: [0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1] \nFitness: 1.3130824248665481e+45\nnew fitness scores: [-6.81567116e+02 -7.18582710e+03 -3.01970933e+50 -2.11287361e+03\n -1.29177706e+51 -9.61358205e+02 -1.70725225e+48 -1.31308242e+45]\nbest chromosome in generation 0 is [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1] with accuracy -681.567115914516\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenetic_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSPARSITY_RATE\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[56], line 18\u001b[0m, in \u001b[0;36mgenetic_algorithm\u001b[0;34m(num_generations, desired_sparsity)\u001b[0m\n\u001b[1;32m     16\u001b[0m parent1, parent2 \u001b[38;5;241m=\u001b[39m parents[i], parents[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     17\u001b[0m child1, child2 \u001b[38;5;241m=\u001b[39m crossover(parent1, parent2)\n\u001b[0;32m---> 18\u001b[0m child1 \u001b[38;5;241m=\u001b[39m \u001b[43mmutate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m child2 \u001b[38;5;241m=\u001b[39m mutate(child2)\n\u001b[1;32m     20\u001b[0m new_population\u001b[38;5;241m.\u001b[39mextend([child1, child2])\n","Cell \u001b[0;32mIn[54], line 22\u001b[0m, in \u001b[0;36mmutate\u001b[0;34m(chromosome)\u001b[0m\n\u001b[1;32m     20\u001b[0m     chromosome[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Flip 1 to 0 only if there are too many 1s\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     num_ones \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m chromosome[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_ones \u001b[38;5;241m<\u001b[39m \u001b[43mtarget_ones\u001b[49m:\n\u001b[1;32m     23\u001b[0m     chromosome[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Flip 0 to 1 only if there are too few 1s\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     num_ones \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","\u001b[0;31mNameError\u001b[0m: name 'target_ones' is not defined"],"ename":"NameError","evalue":"name 'target_ones' is not defined","output_type":"error"}],"execution_count":57}]}