# GenPruning

### Large Language Models demonstrate exceptional capabilities in language comprehension and generation, but their significant size presents challenges for deployment and inference. This research investigates the compression of LLMs by employing structural pruning, specifically targeting the removal of redundant modules like attention heads or MLP layers. This process is guided by a novel application of genetic algorithms that efficiently search for optimal submodels within a vast combinatorial space. The fitness of submodels is assessed based on performance metrics, ensuring minimal degradation from the original LLM. A post-training phase utilizing a smaller dataset facilitates knowledge recovery and further refines the compressed model for enhanced efficiency. This innovative approach aims to democratize access to LLMs by enabling their deployment on devices with limited resources.
