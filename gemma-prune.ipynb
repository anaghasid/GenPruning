{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11384,"sourceType":"modelInstanceVersion","modelInstanceId":6216,"modelId":3301}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-14T12:37:40.066439Z","iopub.execute_input":"2024-10-14T12:37:40.067305Z","iopub.status.idle":"2024-10-14T12:37:40.443679Z","shell.execute_reply.started":"2024-10-14T12:37:40.067257Z","shell.execute_reply":"2024-10-14T12:37:40.442795Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/gemma/transformers/2b/2/model.safetensors.index.json\n/kaggle/input/gemma/transformers/2b/2/gemma-2b.gguf\n/kaggle/input/gemma/transformers/2b/2/config.json\n/kaggle/input/gemma/transformers/2b/2/model-00001-of-00002.safetensors\n/kaggle/input/gemma/transformers/2b/2/model-00002-of-00002.safetensors\n/kaggle/input/gemma/transformers/2b/2/tokenizer.json\n/kaggle/input/gemma/transformers/2b/2/tokenizer_config.json\n/kaggle/input/gemma/transformers/2b/2/special_tokens_map.json\n/kaggle/input/gemma/transformers/2b/2/.gitattributes\n/kaggle/input/gemma/transformers/2b/2/tokenizer.model\n/kaggle/input/gemma/transformers/2b/2/generation_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_json(\"hf://datasets/giuliadc/cnndm_5k/cnndm_5k_2-0.630-test.json\")","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:37:40.445267Z","iopub.execute_input":"2024-10-14T12:37:40.445680Z","iopub.status.idle":"2024-10-14T12:37:41.726829Z","shell.execute_reply.started":"2024-10-14T12:37:40.445645Z","shell.execute_reply":"2024-10-14T12:37:41.726019Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def make_model():\n    try:\n        del model\n        for _ in range(10):\n            torch.cuda.empty_cache()\n            gc.collect()\n    except:\n        pass\n    model = GemmaForCausalLM.from_pretrained(model_name)  # Use the appropriate model class\n    model.to(device)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:01:30.907737Z","iopub.execute_input":"2024-10-14T13:01:30.908300Z","iopub.status.idle":"2024-10-14T13:01:30.913668Z","shell.execute_reply.started":"2024-10-14T13:01:30.908256Z","shell.execute_reply":"2024-10-14T13:01:30.912691Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModel, GemmaForCausalLM \nimport torch\nmodel_name = '/kaggle/input/gemma/transformers/2b/2'  \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = make_model()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:37:41.734690Z","iopub.execute_input":"2024-10-14T12:37:41.735007Z","iopub.status.idle":"2024-10-14T12:38:15.597334Z","shell.execute_reply.started":"2024-10-14T12:37:41.734964Z","shell.execute_reply":"2024-10-14T12:38:15.596291Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b06345bc79145489466a84be65ab0e2"}},"metadata":{}}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:38:15.598652Z","iopub.execute_input":"2024-10-14T12:38:15.599147Z","iopub.status.idle":"2024-10-14T12:38:15.614751Z","shell.execute_reply.started":"2024-10-14T12:38:15.599112Z","shell.execute_reply":"2024-10-14T12:38:15.613793Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        id                                               text  \\\n0  cnndm-1  A flickering rainbow of colours came with the ...   \n1  cnndm-2  Ed Sheeran surprised a fan in hospital for her...   \n2  cnndm-3  It’s been one of the most indulgent shopping s...   \n3  cnndm-4  The Block’s up market South Yarra apartments a...   \n4  cnndm-5  (CNN)Recently, a New York judge issued an opin...   \n\n                                   reference-summary  \\\n0  Italian photographer Giovanna Griffo, 42, shar...   \n1  Jess Knight, 20, was surprised by Ed Sheeran a...   \n2  Lily James, the British star of the upcoming C...   \n3  The Block’s four apartments are now on the mar...   \n4  A court allowed a wife to serve divorce papers...   \n\n                                     source  \n0  4ab4bb5edcb0a9b97b87a0c4a9a721984210c2f4  \n1  ac1b0180881c97d9f0d57d403c0bbc21fe29dd3c  \n2  8f3a478ae49dfe072a204b60c2577c2375341628  \n3  3f87b31923a09bf2248f7b3bc3303bb0c9ebdbe4  \n4  8610ead0a43054be202a1fc756620415a6d572fe  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>reference-summary</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cnndm-1</td>\n      <td>A flickering rainbow of colours came with the ...</td>\n      <td>Italian photographer Giovanna Griffo, 42, shar...</td>\n      <td>4ab4bb5edcb0a9b97b87a0c4a9a721984210c2f4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cnndm-2</td>\n      <td>Ed Sheeran surprised a fan in hospital for her...</td>\n      <td>Jess Knight, 20, was surprised by Ed Sheeran a...</td>\n      <td>ac1b0180881c97d9f0d57d403c0bbc21fe29dd3c</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cnndm-3</td>\n      <td>It’s been one of the most indulgent shopping s...</td>\n      <td>Lily James, the British star of the upcoming C...</td>\n      <td>8f3a478ae49dfe072a204b60c2577c2375341628</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cnndm-4</td>\n      <td>The Block’s up market South Yarra apartments a...</td>\n      <td>The Block’s four apartments are now on the mar...</td>\n      <td>3f87b31923a09bf2248f7b3bc3303bb0c9ebdbe4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cnndm-5</td>\n      <td>(CNN)Recently, a New York judge issued an opin...</td>\n      <td>A court allowed a wife to serve divorce papers...</td>\n      <td>8610ead0a43054be202a1fc756620415a6d572fe</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install bert-score\n!pip install rouge-score\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge_score import rouge_scorer\nimport nltk\nfrom transformers.modeling_utils import prune_linear_layer\nfrom collections import defaultdict\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:38:15.616105Z","iopub.execute_input":"2024-10-14T12:38:15.616422Z","iopub.status.idle":"2024-10-14T12:38:43.490863Z","shell.execute_reply.started":"2024-10-14T12:38:15.616362Z","shell.execute_reply":"2024-10-14T12:38:43.489949Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.4.0)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.2.2)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.44.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.66.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert-score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert-score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert-score) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.25.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.19.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (10.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (2024.8.30)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert-score\nSuccessfully installed bert-score-0.3.13\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=d61f6488b8ebcaab3711cca1f543c587b1c1239ceea0f2074d30a2828edf6125\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Download required packages for BLEU\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:38:43.492191Z","iopub.execute_input":"2024-10-14T12:38:43.492840Z","iopub.status.idle":"2024-10-14T12:38:43.641812Z","shell.execute_reply.started":"2024-10-14T12:38:43.492803Z","shell.execute_reply":"2024-10-14T12:38:43.640903Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"def summarize_text(text, model):\n    inputs = tokenizer(text, return_tensors='pt', max_length=1024, truncation=True).to(\"cuda\")\n    \n    # Use max_new_tokens to control the output length while keeping input handling flexible\n    summary_ids = model.generate(\n        inputs.input_ids, \n        max_new_tokens=150,  # Generates up to 150 tokens in the summary\n        min_length=50, \n        length_penalty=2.0, \n        num_beams=4, \n        early_stopping=True\n    )\n    res = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    del inputs\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:50:50.292863Z","iopub.execute_input":"2024-10-14T12:50:50.293754Z","iopub.status.idle":"2024-10-14T12:50:50.299664Z","shell.execute_reply.started":"2024-10-14T12:50:50.293709Z","shell.execute_reply":"2024-10-14T12:50:50.298692Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# from transformers import AutoTokenizer, GemmaForCausalLM\n# Take the first 100 rows\n\ndf_100 = df.head(30)\n# df_100.to(device)\n# Apply the summarization to the 'text' column for the first 100 rows\ndf_100['generated_summary'] = df_100['text'].apply(summarize_text, model=model)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:38:43.650216Z","iopub.execute_input":"2024-10-14T12:38:43.650527Z","iopub.status.idle":"2024-10-14T12:41:30.727258Z","shell.execute_reply.started":"2024-10-14T12:38:43.650496Z","shell.execute_reply":"2024-10-14T12:41:30.726268Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/393478883.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_100['generated_summary'] = df_100['text'].apply(summarize_text)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize ROUGE scorer\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n# Function to compute BLEU and ROUGE scores\ndef compute_scores(reference, hypothesis):\n    # BLEU score\n    bleu_score = sentence_bleu([reference.split()], hypothesis.split())\n    \n    # ROUGE score\n    rouge_scores = scorer.score(reference, hypothesis)\n    \n    return bleu_score, rouge_scores\n\n# List to store scores\nresults = []\n\n# Iterate through the first 100 rows to compute BLEU and ROUGE scores\nfor i, row in df_100.iterrows():\n    reference = row['reference-summary']\n    hypothesis = row['generated_summary']\n    \n    # Calculate BLEU and ROUGE scores\n    bleu_score, rouge_scores = compute_scores(reference, hypothesis)\n    \n    # Store results\n    results.append({\n        'id': row['id'],\n        'bleu_score': bleu_score,\n        'rouge1': rouge_scores['rouge1'].fmeasure,\n        'rouge2': rouge_scores['rouge2'].fmeasure,\n        'rougeL': rouge_scores['rougeL'].fmeasure\n    })\n\n# Convert results to DataFrame\nresults_df = pd.DataFrame(results)\n\n# Display results\nprint(results_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:41:30.732579Z","iopub.execute_input":"2024-10-14T12:41:30.733077Z","iopub.status.idle":"2024-10-14T12:41:32.118577Z","shell.execute_reply.started":"2024-10-14T12:41:30.733043Z","shell.execute_reply":"2024-10-14T12:41:32.117599Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"        id  bleu_score    rouge1    rouge2    rougeL\n0  cnndm-1    0.012572  0.093126  0.044444  0.082040\n1  cnndm-2    0.024412  0.154278  0.067511  0.109397\n2  cnndm-3    0.012623  0.113350  0.045455  0.080605\n3  cnndm-4    0.019106  0.147087  0.065072  0.093601\n4  cnndm-5    0.067139  0.032020  0.007407  0.022167\n","output_type":"stream"}]},{"cell_type":"code","source":"from bert_score import score\n\n# Assuming df_100 has 'generated_summary' and 'reference-summary' columns\ngenerated_summaries = df_100['generated_summary'].tolist()\nreference_summaries = df_100['reference-summary'].tolist()\n\n# Calculate BERTScore\nP, R, F1 = score(generated_summaries, reference_summaries, lang='en', verbose=True)\n\n# Output the scores\nprint(f'Precision: {P.mean().item():.4f}')\nprint(f'Recall: {R.mean().item():.4f}')\nprint(f'F1 Score: {F1.mean().item():.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:41:32.119849Z","iopub.execute_input":"2024-10-14T12:41:32.120221Z","iopub.status.idle":"2024-10-14T12:41:42.779061Z","shell.execute_reply.started":"2024-10-14T12:41:32.120185Z","shell.execute_reply":"2024-10-14T12:41:42.777884Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c55e79693a384f3eb0752a657eb2b4c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25c6833775dd4db4938c974385ab943d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"763efca7b4b54669be52281c07ed81e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1760ba79e374aad9476e3da66efd3ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49b61f3561ba4d14a2f79d6389d594eb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e92b25319434453abd9b09d9f0fe4e10"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5e3df5d82554b7cbd6030308bfb890d"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd8db408493549b4bb798611929f0ff0"}},"metadata":{}},{"name":"stdout","text":"done in 3.14 seconds, 9.56 sentences/sec\nPrecision: 0.8032\nRecall: 0.8843\nF1 Score: 0.8418\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\ndef create_random_binary_list(length, percentage_of_zeros):\n    num_zeros = int(length * percentage_of_zeros)\n    num_ones = length - num_zeros\n\n    # Create the list with the required number of 0s and 1s\n    binary_list = [0] * num_zeros + [1] * num_ones\n\n    # Shuffle the list to randomize the order\n    random.shuffle(binary_list)\n\n    return binary_list\n\ndef initialize_chromosome(num_genes):\n  # initialize chromosome with given sparsity percentage\n  return create_random_binary_list(num_genes, SPARSITY_RATE)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:41:42.780551Z","iopub.execute_input":"2024-10-14T12:41:42.781247Z","iopub.status.idle":"2024-10-14T12:41:42.786913Z","shell.execute_reply.started":"2024-10-14T12:41:42.781195Z","shell.execute_reply":"2024-10-14T12:41:42.786012Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def modify_model(model, chromosome):\n    num_heads = model.config.num_attention_heads\n    num_blocks = model.config.num_hidden_layers\n    # num_ffn = model.config.num_hidden_layers\n\n    # Disable attention heads\n    heads_to_prune = defaultdict(list)\n    for i, gene in enumerate(chromosome):\n        if gene == 0:\n          block_num = i//num_heads\n          head_num = i%num_heads\n          heads_to_prune[block_num].append(head_num)\n\n    head_dim = model.config.head_dim\n    if heads_to_prune:\n        print(\"Pruning heads in model\")\n        with torch.no_grad():\n            for block in range(block_num): \n                for head in heads_to_prune[block]:\n                        # Zero-out the corresponding rows in the q_proj, k_proj, and v_proj\n                        start_index = head * head_dim\n                        end_index = (head + 1) * head_dim\n                        model.model.layers[block].self_attn.q_proj.weight[start_index:end_index, :] = 0\n                        model.model.layers[block].self_attn.k_proj.weight[start_index:end_index, :] = 0\n                        model.model.layers[block].self_attn.v_proj.weight[start_index:end_index, :] = 0\n\n    #                     model.model.layers[block_num].self_attn.q_proj.weight = model.model.layers[block_num].self_attn.q_proj.weight.to(torch.int8)\n    #                     model.model.layers[block_num].self_attn.k_proj.weight = model.model.layers[block_num].self_attn.q_proj.weight.to(torch.int8)\n    #                     model.model.layers[block_num].self_attn.c_proj.weight = model.model.layers[block_num].self_attn.q_proj.weight.to(torch.int8)\n\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:41:42.788044Z","iopub.execute_input":"2024-10-14T12:41:42.788327Z","iopub.status.idle":"2024-10-14T12:41:44.601294Z","shell.execute_reply.started":"2024-10-14T12:41:42.788295Z","shell.execute_reply":"2024-10-14T12:41:44.600210Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def evaluate_fitness(chromosome, dataset):\n    model = make_model()\n    model = modify_model(model,chromosome)\n    df_100['generated_summary'] = dataset['text'].apply(summarize_text, model=model)\n    df_100['reference-summary'] = dataset['reference-summary']\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n    # List to store scores\n    results = []\n    \n    # Iterate through the first 100 rows to compute BLEU and ROUGE scores\n    for i, row in df_100.iterrows():\n        reference = row['reference-summary']\n        hypothesis = row['generated_summary']\n        \n        # Calculate BLEU and ROUGE scores\n        bleu_score, rouge_scores = compute_scores(reference, hypothesis)\n        \n        # Store results\n        results.append({\n            'id': row['id'],\n            'bleu_score': bleu_score,\n            'rouge1': rouge_scores['rouge1'].fmeasure,\n            'rouge2': rouge_scores['rouge2'].fmeasure,\n            'rougeL': rouge_scores['rougeL'].fmeasure\n        })\n    \n    # Convert results to DataFrame\n    results_df = pd.DataFrame(results)\n    generated_summaries = df_100['generated_summary'].tolist()\n    reference_summaries = df_100['reference-summary'].tolist()\n\n    # Calculate BERTScore\n    P, R, F1 = score(generated_summaries, reference_summaries, lang='en', verbose=True)\n    return P.mean().item()\n\n\ndef compute_scores(reference, hypothesis):\n    # BLEU score\n    bleu_score = sentence_bleu([reference.split()], hypothesis.split())\n    \n    # ROUGE score\n    rouge_scores = scorer.score(reference, hypothesis)\n    \n    return bleu_score, rouge_scores","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:58:47.737832Z","iopub.execute_input":"2024-10-14T12:58:47.738569Z","iopub.status.idle":"2024-10-14T12:58:47.748846Z","shell.execute_reply.started":"2024-10-14T12:58:47.738526Z","shell.execute_reply":"2024-10-14T12:58:47.747674Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"SPARSITY_RATE = 0.3","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:41:44.694683Z","iopub.execute_input":"2024-10-14T12:41:44.695080Z","iopub.status.idle":"2024-10-14T12:41:44.823816Z","shell.execute_reply.started":"2024-10-14T12:41:44.695035Z","shell.execute_reply":"2024-10-14T12:41:44.822537Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def find_size(model):\n  total_size_in_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n  total_size_in_megabytes = total_size_in_bytes / (1024 ** 2)\n  print(f\"Model size: {total_size_in_megabytes:.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:41:44.825186Z","iopub.execute_input":"2024-10-14T12:41:44.825789Z","iopub.status.idle":"2024-10-14T12:41:44.905167Z","shell.execute_reply.started":"2024-10-14T12:41:44.825751Z","shell.execute_reply":"2024-10-14T12:41:44.904044Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = make_model()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:46:05.151377Z","iopub.execute_input":"2024-10-14T12:46:05.152184Z","iopub.status.idle":"2024-10-14T12:46:12.554512Z","shell.execute_reply.started":"2024-10-14T12:46:05.152143Z","shell.execute_reply":"2024-10-14T12:46:12.553678Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aed271295cb470695b9e938882274f9"}},"metadata":{}}]},{"cell_type":"code","source":"chrom = initialize_chromosome(model.config.num_attention_heads*model.config.num_hidden_layers)\n# model = modify_model(model, chrom)\n# find_size(model)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:46:12.556106Z","iopub.execute_input":"2024-10-14T12:46:12.556444Z","iopub.status.idle":"2024-10-14T12:46:12.560974Z","shell.execute_reply.started":"2024-10-14T12:46:12.556408Z","shell.execute_reply":"2024-10-14T12:46:12.560065Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import gc\ndel model\ndel df\nfor _ in range(10):\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:46:34.011057Z","iopub.execute_input":"2024-10-14T12:46:34.011454Z","iopub.status.idle":"2024-10-14T12:46:35.925218Z","shell.execute_reply.started":"2024-10-14T12:46:34.011408Z","shell.execute_reply":"2024-10-14T12:46:35.924165Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:46:12.604870Z","iopub.status.idle":"2024-10-14T12:46:12.605208Z","shell.execute_reply.started":"2024-10-14T12:46:12.605033Z","shell.execute_reply":"2024-10-14T12:46:12.605050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = make_model()\n# find_size(model)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:46:12.606830Z","iopub.status.idle":"2024-10-14T12:46:12.607301Z","shell.execute_reply.started":"2024-10-14T12:46:12.607051Z","shell.execute_reply":"2024-10-14T12:46:12.607075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_fitness(chrom, df_100)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:51:17.250493Z","iopub.execute_input":"2024-10-14T12:51:17.250861Z","iopub.status.idle":"2024-10-14T12:54:31.383360Z","shell.execute_reply.started":"2024-10-14T12:51:17.250829Z","shell.execute_reply":"2024-10-14T12:54:31.382445Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ac0f39851cd459b9d15fbde0ed501e4"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a40a523464447c892cd590140e85e11"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da1a5f8fe3074f37904d4f531b1aa4aa"}},"metadata":{}},{"name":"stdout","text":"done in 3.06 seconds, 9.80 sentences/sec\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"0.8033027648925781"},"metadata":{}}]},{"cell_type":"code","source":"POPN_SIZE = 8\ncrossover_rate = 0.7\nmutation_rate = 0.08\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:55:12.160230Z","iopub.execute_input":"2024-10-14T12:55:12.160657Z","iopub.status.idle":"2024-10-14T12:55:12.165440Z","shell.execute_reply.started":"2024-10-14T12:55:12.160610Z","shell.execute_reply":"2024-10-14T12:55:12.164424Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def initialize_population(chromosome_length):\n  # initialize random population\n  population = []\n  for _ in range(POPN_SIZE):\n    chromosome = initialize_chromosome(chromosome_length)\n    population.append(chromosome)\n  return population","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:55:12.167297Z","iopub.execute_input":"2024-10-14T12:55:12.167706Z","iopub.status.idle":"2024-10-14T12:55:12.177411Z","shell.execute_reply.started":"2024-10-14T12:55:12.167657Z","shell.execute_reply":"2024-10-14T12:55:12.176536Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def select_parents(population, fitness_scores, num_parents):\n    # Normalize fitness scores to create a probability distribution\n    total_fitness = np.sum(fitness_scores)\n    probabilities = fitness_scores / total_fitness\n\n    # Select parents based on their fitness proportion (roulette wheel selection)\n    # selected_parents = np.random.choice(population, size=num_parents, p=probabilities, replace=True)\n    selected_parents = random.choices(population, weights=probabilities, k=num_parents)\n\n\n    return np.array(selected_parents)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:55:12.179269Z","iopub.execute_input":"2024-10-14T12:55:12.179641Z","iopub.status.idle":"2024-10-14T12:55:12.191995Z","shell.execute_reply.started":"2024-10-14T12:55:12.179598Z","shell.execute_reply":"2024-10-14T12:55:12.191111Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Crossover (Single-point crossover)\ndef crossover(parent1, parent2):\n    if np.random.rand() < crossover_rate:\n        point = np.random.randint(1, len(parent1) - 1)\n        child1 = np.concatenate([parent1[:point], parent2[point:]])\n        child2 = np.concatenate([parent2[:point], parent1[point:]])\n    else:\n        child1, child2 = parent1, parent2\n    return child1, child2","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:55:12.193119Z","iopub.execute_input":"2024-10-14T12:55:12.193444Z","iopub.status.idle":"2024-10-14T12:55:12.201614Z","shell.execute_reply.started":"2024-10-14T12:55:12.193411Z","shell.execute_reply":"2024-10-14T12:55:12.200758Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Mutation (Flip bit mutation)\ndef mutate(chromosome):\n    for i in range(len(chromosome)):\n        if np.random.rand() < mutation_rate:\n            chromosome[i] = 1 - chromosome[i]\n\n    target_zeros = int(len(chromosome) * SPARSITY_RATE)\n\n    for c in range(model.config.num_attention_heads-1, len(chromosome), model.config.num_attention_heads):\n      # this part to ensure that each layer has at least one attention head\n      start = c-model.config.num_attention_heads-1\n      enc_part = chromosome[start:c]\n      num_ones = np.sum(enc_part)  # Count the number of 1s in the chromosome\n      if num_ones==0:\n        chromosome[start] = 1\n\n    for i in range(len(chromosome)):\n        if np.random.rand() < mutation_rate:\n            if chromosome[i] == 1 and num_ones > target_ones:\n                chromosome[i] = 0  # Flip 1 to 0 only if there are too many 1s\n                num_ones -= 1\n            elif chromosome[i] == 0 and num_ones < target_ones:\n                chromosome[i] = 1  # Flip 0 to 1 only if there are too few 1s\n                num_ones += 1\n    return chromosome\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:55:12.203546Z","iopub.execute_input":"2024-10-14T12:55:12.203893Z","iopub.status.idle":"2024-10-14T12:55:12.214735Z","shell.execute_reply.started":"2024-10-14T12:55:12.203861Z","shell.execute_reply":"2024-10-14T12:55:12.213929Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def elitism_and_selection(population, fitness_scores, num_elites, num_parents):\n    # Elitism: Keep the top num_elites individuals\n    elite_indices = np.argsort(fitness_scores)[-num_elites:]  # Get indices of top individuals\n    elites = [population[i] for i in elite_indices]\n\n    # Perform roulette wheel selection for the rest of the parents\n    remaining_population = np.delete(population, elite_indices, axis=0)\n    remaining_fitness_scores = np.delete(fitness_scores, elite_indices)\n\n    num_to_select = num_parents - num_elites\n    selected_parents = select_parents(remaining_population, remaining_fitness_scores, num_to_select)\n\n    # Combine elites and selected parents\n    next_generation = np.vstack((elites, selected_parents))\n\n    return next_generation","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:55:12.215712Z","iopub.execute_input":"2024-10-14T12:55:12.215983Z","iopub.status.idle":"2024-10-14T12:55:12.228002Z","shell.execute_reply.started":"2024-10-14T12:55:12.215952Z","shell.execute_reply":"2024-10-14T12:55:12.227134Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"model = make_model()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:03:12.823559Z","iopub.execute_input":"2024-10-14T13:03:12.823936Z","iopub.status.idle":"2024-10-14T13:03:19.711006Z","shell.execute_reply.started":"2024-10-14T13:03:12.823902Z","shell.execute_reply":"2024-10-14T13:03:19.709939Z"},"trusted":true},"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fb5e6e60fe643438aeb1724f45ac40a"}},"metadata":{}}]},{"cell_type":"code","source":"num_attention_heads = model.config.num_attention_heads\nnum_hidden_layers = model.config.num_hidden_layers","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:03:23.095408Z","iopub.execute_input":"2024-10-14T13:03:23.096012Z","iopub.status.idle":"2024-10-14T13:03:23.100416Z","shell.execute_reply.started":"2024-10-14T13:03:23.095969Z","shell.execute_reply":"2024-10-14T13:03:23.099382Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef genetic_algorithm(num_generations, desired_sparsity):\n  population = initialize_population(num_attention_heads*num_hidden_layers)  # Initialize the population\n  accuracy_per_generation = []  # List to store highest accuracy values for each generation\n  for generation in range(num_generations):\n      fitness_scores = np.array([evaluate_fitness(chrom, df_100) for chrom in population])\n      best_chromosome = population[np.argmax(fitness_scores)]\n      print(\"new fitness scores:\", fitness_scores)\n      print(f\"best chromosome in generation {generation} is {best_chromosome} with accuracy {fitness_scores[np.argmax(fitness_scores)]}\")\n      accuracy_per_generation.append(fitness_scores[np.argmax(fitness_scores)])\n      parents = elitism_and_selection(population, fitness_scores, 4, POPN_SIZE)\n      # parents = select_parents(population, fitness_scores, POPN_SIZE)\n      new_population = []\n      for i in range(0, POPN_SIZE, 2):\n          parent1, parent2 = parents[i], parents[i + 1]\n          child1, child2 = crossover(parent1, parent2)\n          # child1 = mutate(child1)\n          # child2 = mutate(child2)\n          new_population.extend([child1, child2])\n      population = np.array(new_population)\n  generations = list(range(1, len(accuracy_per_generation) + 1))\n\n\n  return best_chromosome","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:03:49.245043Z","iopub.execute_input":"2024-10-14T13:03:49.245417Z","iopub.status.idle":"2024-10-14T13:03:49.254774Z","shell.execute_reply.started":"2024-10-14T13:03:49.245370Z","shell.execute_reply":"2024-10-14T13:03:49.253773Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"genetic_algorithm(2, SPARSITY_RATE)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:55.272781Z","iopub.execute_input":"2024-10-14T13:04:55.273168Z","iopub.status.idle":"2024-10-14T15:06:51.444375Z","shell.execute_reply.started":"2024-10-14T13:04:55.273131Z","shell.execute_reply":"2024-10-14T15:06:51.443492Z"},"trusted":true},"execution_count":66,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aeed1a75e8a40dfac64a5580e4bdb14"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c47dff916514e5ca8571a553fa99af9"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b620987736104883b288f313f8f4a6cf"}},"metadata":{}},{"name":"stdout","text":"done in 2.97 seconds, 10.10 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"797ce8bcb80b4dd5adfa0863819d2f3d"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"895e11f2c014455eb91f0a7b99cf6750"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cab39cd7f7243b2a6bf9115d69ca5a1"}},"metadata":{}},{"name":"stdout","text":"done in 2.94 seconds, 10.21 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a66321b64f3a4160ba60c34975ac4913"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a871596b9ad460094871938da7657d2"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e8a27c8384a4bcabeda0cc8a43e68b4"}},"metadata":{}},{"name":"stdout","text":"done in 2.94 seconds, 10.19 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b7b823bca7041929376f7c7032680be"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a3e64eb5abe4eda8bf9700158995280"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"479d1dc90aa54ad8a65559bd01707ce8"}},"metadata":{}},{"name":"stdout","text":"done in 2.93 seconds, 10.25 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1489badec064f5c8687452cb1f81efc"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f34b426906f484881cc70ed266c0e05"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"194809a2afd5451e8457372bfb48a096"}},"metadata":{}},{"name":"stdout","text":"done in 2.97 seconds, 10.10 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"587e5aaaf55546fb81793d1e8a0750ba"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b991c8d85f6448d1bb88379670d4c3c2"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36777d7dd5c84d4d9ded7405fc82f27c"}},"metadata":{}},{"name":"stdout","text":"done in 2.96 seconds, 10.13 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5726a266cefa4a258ead48443bc18c75"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8683fce6baed46c8882ae4c145884ff1"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a86bc3abc2e4405a651a536a573f3b8"}},"metadata":{}},{"name":"stdout","text":"done in 2.97 seconds, 10.12 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53fbaa358d0940e8acd84ef93d7d717c"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2285d6881f0d442da00b3b1c28fc31b9"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"296d0f55b0c14ebe9351579941f5c681"}},"metadata":{}},{"name":"stdout","text":"done in 2.95 seconds, 10.15 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9b9a2f74fe9474b84a8426ad5d8e5dc"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62b9066862aa4016a3b2e1d66518523e"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f225c4bb7aa40328857e408358a4932"}},"metadata":{}},{"name":"stdout","text":"done in 2.95 seconds, 10.17 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9396ad268814cbaaf21ab787fc26878"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26f89e36bce94c3bac2c7c11a0a298a3"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60f196c675ce45dc89b0657de494a5eb"}},"metadata":{}},{"name":"stdout","text":"done in 2.96 seconds, 10.15 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff836a129df74213b927ff7a0a6f4ed8"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4b1bb407c7d41379565af4c6f0fc5e0"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67ac7e16d2104cdf941b92468b6db491"}},"metadata":{}},{"name":"stdout","text":"done in 2.93 seconds, 10.23 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc04842f4b64450d92e849b6a07a486c"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ff2ff2a3aab4306b9d5563420f50e4b"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf8a82c9a4b24e47a18e357f4b9082af"}},"metadata":{}},{"name":"stdout","text":"done in 2.90 seconds, 10.33 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65abd4b2d7444e1384bc02593b6a3e63"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9617f8602cee4ca4aa94c93903df9c93"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20ecb74c9fe14c6ba02fc5dcd6655953"}},"metadata":{}},{"name":"stdout","text":"done in 2.90 seconds, 10.35 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02fac87174c14949a0968be758907d67"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ae6e19df18548008fe4707c17a7d649"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cedfe71a02e4e808915e897ad033e56"}},"metadata":{}},{"name":"stdout","text":"done in 2.93 seconds, 10.25 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0611d8d77c5e4974b2fb89fda2a460eb"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67253688398e47dda3094441825fc370"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76e7f7afb8184418b82596ebd300d360"}},"metadata":{}},{"name":"stdout","text":"done in 2.97 seconds, 10.12 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0224110f549045048bbb900ccf8d5746"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7993836a4f2f424798bb01e77860301e"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1838d501b0e2441d93c95938e36ab8d8"}},"metadata":{}},{"name":"stdout","text":"done in 2.92 seconds, 10.27 sentences/sec\nnew fitness scores: [0.80212402 0.80304563 0.80154383 0.80302864 0.80115384 0.8010596\n 0.8010906  0.80219674 0.80261916 0.8016848  0.8026759  0.8022269\n 0.80316377 0.80200857 0.80110371 0.80199558]\nbest chromosome in generation 0 is [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1] with accuracy 0.8031637668609619\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff0cfb3b2e3b4043b090dbe76c8a4304"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0c9509d406e400abe21843ca5ae3fab"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b9ec501358e45e0928f9be4d509820d"}},"metadata":{}},{"name":"stdout","text":"done in 2.92 seconds, 10.29 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"889ba969e4a943a1b364e7e302a262af"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60f39e6771a948a7a9d3d7c3cb0670cd"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9105cd0fa0174a40a790010a9a533212"}},"metadata":{}},{"name":"stdout","text":"done in 2.93 seconds, 10.22 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6fd17838a904fbcaab45527cc373e65"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da835b5b65544c7f83daffcdc1ccaba6"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d92d5d5d34456c87e5fd64d44b90de"}},"metadata":{}},{"name":"stdout","text":"done in 2.91 seconds, 10.32 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be380122de2e4f9688ca6b0603389c8c"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a169a89030d74185bb7372c9fbab9a74"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb920c142c204788a148c7d170a6b437"}},"metadata":{}},{"name":"stdout","text":"done in 2.92 seconds, 10.27 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a61a64658a9045ab90c56c350b2aa812"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e77aefb5b8e4573b7ca304e00368808"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92daa4547ee44490bb98aff34da369fe"}},"metadata":{}},{"name":"stdout","text":"done in 2.97 seconds, 10.09 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5707e92e6f5435fae1f6f268ef6bae5"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a6e957b5fad4772b2d9fedd90f419c3"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdd03196bd1746c0a218eac911cdbb4c"}},"metadata":{}},{"name":"stdout","text":"done in 2.95 seconds, 10.18 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9972d3d5086249e2b69522e3d15d98cb"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b904e9df4de4d22869fc9317d36e869"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4bc82c930f44474bb186ec90429994a"}},"metadata":{}},{"name":"stdout","text":"done in 2.97 seconds, 10.09 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2464e5c65fa54185848c05d2e98cc07c"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e98f78e4b4fc4a1bb60aabb6670d9749"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddfb477dacb744a7973d2849d9f1b664"}},"metadata":{}},{"name":"stdout","text":"done in 2.96 seconds, 10.14 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ca15286a7334c658040f190b958a872"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cc3558a54b5467a95904dc083b2e1da"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"488cae3157c646c2bc8c4f697a8e1ea1"}},"metadata":{}},{"name":"stdout","text":"done in 2.92 seconds, 10.27 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3861b91bac6541f2bd8ec115cc6ea8eb"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f9133e983e6446697e134c171f09037"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91859115b6b5404cafbe771b9fa58c42"}},"metadata":{}},{"name":"stdout","text":"done in 2.97 seconds, 10.12 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e6e6f80c35a4ba2bc2420de4d8bd459"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d92ed670e67a487cb1ede019bb7f5d41"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edffdf6e73274115b3ac9a0d40d052fd"}},"metadata":{}},{"name":"stdout","text":"done in 2.96 seconds, 10.14 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9135aed52f54c7287550fc338d207f9"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cafe53e7b66b4c7cb59e19c07492c76b"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"161977582d2445e8b00fff7134d0be6e"}},"metadata":{}},{"name":"stdout","text":"done in 2.97 seconds, 10.11 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c23e9cdaf08452490d927741aae2a22"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc9225b3f0824022acf49884bf8e53c1"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fdb6dd3498940d088ab5191516b54a4"}},"metadata":{}},{"name":"stdout","text":"done in 2.94 seconds, 10.20 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67a226bc462645b5be76ae9d68219db7"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9519184a43bc460898b985d37389245d"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7be71ba9db574d11a10504ef00e1258b"}},"metadata":{}},{"name":"stdout","text":"done in 2.96 seconds, 10.14 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bb81e6479924173962dc4f396254208"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3620dc828d24ef6b671bbaeafaeea4e"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92466685a8fa45a3b14f3cafa91e9810"}},"metadata":{}},{"name":"stdout","text":"done in 2.93 seconds, 10.25 sentences/sec\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aed18e13bbe44043b5faaafae41c8ccb"}},"metadata":{}},{"name":"stdout","text":"Pruning heads in model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19c80f64799e47e2b304e1f4b68851bf"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7408b73ab7264deaa5f172a043ceca44"}},"metadata":{}},{"name":"stdout","text":"done in 2.96 seconds, 10.14 sentences/sec\nnew fitness scores: [0.8026759  0.80302864 0.80277288 0.80306411 0.80203593 0.80127829\n 0.80115384 0.80115384 0.80230469 0.8010906  0.80110371 0.80110371\n 0.80250347 0.80101788 0.80230826 0.8010596 ]\nbest chromosome in generation 1 is [1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0\n 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1\n 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1\n 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1] with accuracy 0.8030641078948975\n","output_type":"stream"},{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"array([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n       0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n       0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1])"},"metadata":{}}]}]}