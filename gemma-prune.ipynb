{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11384,"sourceType":"modelInstanceVersion","modelInstanceId":6216,"modelId":3301}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-13T05:48:15.078846Z","iopub.execute_input":"2024-10-13T05:48:15.079279Z","iopub.status.idle":"2024-10-13T05:48:16.063961Z","shell.execute_reply.started":"2024-10-13T05:48:15.079236Z","shell.execute_reply":"2024-10-13T05:48:16.062983Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/gemma/transformers/2b/2/model.safetensors.index.json\n/kaggle/input/gemma/transformers/2b/2/gemma-2b.gguf\n/kaggle/input/gemma/transformers/2b/2/config.json\n/kaggle/input/gemma/transformers/2b/2/model-00001-of-00002.safetensors\n/kaggle/input/gemma/transformers/2b/2/model-00002-of-00002.safetensors\n/kaggle/input/gemma/transformers/2b/2/tokenizer.json\n/kaggle/input/gemma/transformers/2b/2/tokenizer_config.json\n/kaggle/input/gemma/transformers/2b/2/special_tokens_map.json\n/kaggle/input/gemma/transformers/2b/2/.gitattributes\n/kaggle/input/gemma/transformers/2b/2/tokenizer.model\n/kaggle/input/gemma/transformers/2b/2/generation_config.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_json(\"hf://datasets/giuliadc/cnndm_5k/cnndm_5k_2-0.630-test.json\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:48:16.065466Z","iopub.execute_input":"2024-10-13T05:48:16.065851Z","iopub.status.idle":"2024-10-13T05:48:17.411273Z","shell.execute_reply.started":"2024-10-13T05:48:16.065818Z","shell.execute_reply":"2024-10-13T05:48:17.410295Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModel, GemmaForCausalLM \nimport torch\n# Load the tokenizer and the correct model\nmodel_name = '/kaggle/input/gemma/transformers/2b/2'  # Update with your model path\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = GemmaForCausalLM.from_pretrained(model_name)  # Use the appropriate model class\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.to(device)  ","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:48:17.412377Z","iopub.execute_input":"2024-10-13T05:48:17.412713Z","iopub.status.idle":"2024-10-13T05:48:51.868856Z","shell.execute_reply.started":"2024-10-13T05:48:17.412680Z","shell.execute_reply":"2024-10-13T05:48:51.867869Z"},"trusted":true},"outputs":[{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9944672d74674d7facad8f6b955bf3cb"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): PytorchGELUTanh()\n        )\n        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n      )\n    )\n    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n  )\n  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:48:51.870823Z","iopub.execute_input":"2024-10-13T05:48:51.871295Z","iopub.status.idle":"2024-10-13T05:48:51.885679Z","shell.execute_reply.started":"2024-10-13T05:48:51.871262Z","shell.execute_reply":"2024-10-13T05:48:51.884807Z"},"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        id                                               text  \\\n0  cnndm-1  A flickering rainbow of colours came with the ...   \n1  cnndm-2  Ed Sheeran surprised a fan in hospital for her...   \n2  cnndm-3  It’s been one of the most indulgent shopping s...   \n3  cnndm-4  The Block’s up market South Yarra apartments a...   \n4  cnndm-5  (CNN)Recently, a New York judge issued an opin...   \n\n                                   reference-summary  \\\n0  Italian photographer Giovanna Griffo, 42, shar...   \n1  Jess Knight, 20, was surprised by Ed Sheeran a...   \n2  Lily James, the British star of the upcoming C...   \n3  The Block’s four apartments are now on the mar...   \n4  A court allowed a wife to serve divorce papers...   \n\n                                     source  \n0  4ab4bb5edcb0a9b97b87a0c4a9a721984210c2f4  \n1  ac1b0180881c97d9f0d57d403c0bbc21fe29dd3c  \n2  8f3a478ae49dfe072a204b60c2577c2375341628  \n3  3f87b31923a09bf2248f7b3bc3303bb0c9ebdbe4  \n4  8610ead0a43054be202a1fc756620415a6d572fe  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>reference-summary</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cnndm-1</td>\n      <td>A flickering rainbow of colours came with the ...</td>\n      <td>Italian photographer Giovanna Griffo, 42, shar...</td>\n      <td>4ab4bb5edcb0a9b97b87a0c4a9a721984210c2f4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cnndm-2</td>\n      <td>Ed Sheeran surprised a fan in hospital for her...</td>\n      <td>Jess Knight, 20, was surprised by Ed Sheeran a...</td>\n      <td>ac1b0180881c97d9f0d57d403c0bbc21fe29dd3c</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cnndm-3</td>\n      <td>It’s been one of the most indulgent shopping s...</td>\n      <td>Lily James, the British star of the upcoming C...</td>\n      <td>8f3a478ae49dfe072a204b60c2577c2375341628</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cnndm-4</td>\n      <td>The Block’s up market South Yarra apartments a...</td>\n      <td>The Block’s four apartments are now on the mar...</td>\n      <td>3f87b31923a09bf2248f7b3bc3303bb0c9ebdbe4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cnndm-5</td>\n      <td>(CNN)Recently, a New York judge issued an opin...</td>\n      <td>A court allowed a wife to serve divorce papers...</td>\n      <td>8610ead0a43054be202a1fc756620415a6d572fe</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"!pip install bert-score\n!pip install rouge-score\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge_score import rouge_scorer\nimport nltk\nfrom transformers.modeling_utils import prune_linear_layer\nfrom collections import defaultdict\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:48:51.886694Z","iopub.execute_input":"2024-10-13T05:48:51.886982Z","iopub.status.idle":"2024-10-13T05:49:20.121487Z","shell.execute_reply.started":"2024-10-13T05:48:51.886951Z","shell.execute_reply":"2024-10-13T05:49:20.120654Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.4.0)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.2.2)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.44.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.66.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert-score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert-score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert-score) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.25.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.19.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (10.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (2024.8.30)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert-score\nSuccessfully installed bert-score-0.3.13\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=d2f84b2fc77bf311f19b409d6177d947a3f736a3141ba7b8a821204ff026f275\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Download required packages for BLEU\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:49:20.122786Z","iopub.execute_input":"2024-10-13T05:49:20.123430Z","iopub.status.idle":"2024-10-13T05:49:20.268608Z","shell.execute_reply.started":"2024-10-13T05:49:20.123395Z","shell.execute_reply":"2024-10-13T05:49:20.267683Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def summarize_text(text):\n    inputs = tokenizer(text, return_tensors='pt', max_length=1024, truncation=True).to(\"cuda\")\n    # Use max_new_tokens to control the output length while keeping input handling flexible\n    summary_ids = model.generate(\n        inputs.input_ids, \n        max_new_tokens=150,  # Generates up to 150 tokens in the summary\n        min_length=50, \n        length_penalty=2.0, \n        num_beams=4, \n        early_stopping=True\n    )\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:49:20.270284Z","iopub.execute_input":"2024-10-13T05:49:20.270970Z","iopub.status.idle":"2024-10-13T05:49:20.276721Z","shell.execute_reply.started":"2024-10-13T05:49:20.270921Z","shell.execute_reply":"2024-10-13T05:49:20.275639Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoTokenizer, GemmaForCausalLM\n# Take the first 100 rows\n\ndf_100 = df.head(30)\n# df_100.to(device)\n# Apply the summarization to the 'text' column for the first 100 rows\ndf_100['generated_summary'] = df_100['text'].apply(summarize_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:49:20.277928Z","iopub.execute_input":"2024-10-13T05:49:20.278299Z","iopub.status.idle":"2024-10-13T05:52:07.524180Z","shell.execute_reply.started":"2024-10-13T05:49:20.278258Z","shell.execute_reply":"2024-10-13T05:52:07.523077Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/837514614.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_100['generated_summary'] = df_100['text'].apply(summarize_text)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Initialize ROUGE scorer\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n# Function to compute BLEU and ROUGE scores\ndef compute_scores(reference, hypothesis):\n    # BLEU score\n    bleu_score = sentence_bleu([reference.split()], hypothesis.split())\n    \n    # ROUGE score\n    rouge_scores = scorer.score(reference, hypothesis)\n    \n    return bleu_score, rouge_scores\n\n# List to store scores\nresults = []\n\n# Iterate through the first 100 rows to compute BLEU and ROUGE scores\nfor i, row in df_100.iterrows():\n    reference = row['reference-summary']\n    hypothesis = row['generated_summary']\n    \n    # Calculate BLEU and ROUGE scores\n    bleu_score, rouge_scores = compute_scores(reference, hypothesis)\n    \n    # Store results\n    results.append({\n        'id': row['id'],\n        'bleu_score': bleu_score,\n        'rouge1': rouge_scores['rouge1'].fmeasure,\n        'rouge2': rouge_scores['rouge2'].fmeasure,\n        'rougeL': rouge_scores['rougeL'].fmeasure\n    })\n\n# Convert results to DataFrame\nresults_df = pd.DataFrame(results)\n\n# Display results\nprint(results_df.head())\n\n# # Save results to a CSV file\n# results_df.to_csv('/path_to/evaluation_results.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:52:07.525760Z","iopub.execute_input":"2024-10-13T05:52:07.526728Z","iopub.status.idle":"2024-10-13T05:52:08.925848Z","shell.execute_reply.started":"2024-10-13T05:52:07.526679Z","shell.execute_reply":"2024-10-13T05:52:08.924881Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"        id  bleu_score    rouge1    rouge2    rougeL\n0  cnndm-1    0.012572  0.093126  0.044444  0.082040\n1  cnndm-2    0.024412  0.154278  0.067511  0.109397\n2  cnndm-3    0.012623  0.113350  0.045455  0.080605\n3  cnndm-4    0.019106  0.147087  0.065072  0.093601\n4  cnndm-5    0.067139  0.032020  0.007407  0.022167\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from bert_score import score\n\n# Assuming df_100 has 'generated_summary' and 'reference-summary' columns\ngenerated_summaries = df_100['generated_summary'].tolist()\nreference_summaries = df_100['reference-summary'].tolist()\n\n# Calculate BERTScore\nP, R, F1 = score(generated_summaries, reference_summaries, lang='en', verbose=True)\n\n# Output the scores\nprint(f'Precision: {P.mean().item():.4f}')\nprint(f'Recall: {R.mean().item():.4f}')\nprint(f'F1 Score: {F1.mean().item():.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:52:08.929580Z","iopub.execute_input":"2024-10-13T05:52:08.929917Z","iopub.status.idle":"2024-10-13T05:52:21.389967Z","shell.execute_reply.started":"2024-10-13T05:52:08.929886Z","shell.execute_reply":"2024-10-13T05:52:21.389120Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e252f72c3224700a5940addcb60892d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e8b7eaf1c6441e8a621519866d077de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"463c9e94d4674a7b961a8436646a26c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14313b7da7474686be7b7bb6551ee3e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd6370540c4244e8865e0391359752d7"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e469e134ebd5434c9903222a690ca4a8"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4499dcf048ea41d3b92f095c3c195ba0"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8fc332d32e2465887530240d25b42ac"}},"metadata":{}},{"name":"stdout","text":"done in 3.29 seconds, 9.11 sentences/sec\nPrecision: 0.8032\nRecall: 0.8843\nF1 Score: 0.8418\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import random\ndef create_random_binary_list(length, percentage_of_zeros):\n    num_zeros = int(length * percentage_of_zeros)\n    num_ones = length - num_zeros\n\n    # Create the list with the required number of 0s and 1s\n    binary_list = [0] * num_zeros + [1] * num_ones\n\n    # Shuffle the list to randomize the order\n    random.shuffle(binary_list)\n\n    return binary_list\n\ndef initialize_chromosome(num_genes):\n  # initialize chromosome with given sparsity percentage\n  return create_random_binary_list(num_genes, SPARSITY_RATE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T05:52:21.391095Z","iopub.execute_input":"2024-10-13T05:52:21.391409Z","iopub.status.idle":"2024-10-13T05:52:21.396916Z","shell.execute_reply.started":"2024-10-13T05:52:21.391376Z","shell.execute_reply":"2024-10-13T05:52:21.396117Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def make_model():\n    # del model\n    model = GemmaForCausalLM.from_pretrained(model_name)  # Use the appropriate model class\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n        model = torch.nn.DataParallel(model)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T05:52:21.398240Z","iopub.execute_input":"2024-10-13T05:52:21.398528Z","iopub.status.idle":"2024-10-13T05:52:21.602528Z","shell.execute_reply.started":"2024-10-13T05:52:21.398497Z","shell.execute_reply":"2024-10-13T05:52:21.601351Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def modify_model(model, chromosome):\n    num_heads = model.config.num_attention_heads\n    num_blocks = model.config.num_hidden_layers\n    # num_ffn = model.config.num_hidden_layers\n\n    # Disable attention heads\n    heads_to_prune = defaultdict(list)\n    for i, gene in enumerate(chromosome):\n        if gene == 0:\n          block_num = i//num_heads\n          head_num = i%num_heads\n          heads_to_prune[block_num].append(head_num)\n\n    head_dim = model.config.head_dim\n    if heads_to_prune:\n        for block_num in heads_to_prune:\n            keep_indices = []\n            for head in range(num_heads):\n                # Get the range of indices for this head\n                start_idx = head * head_dim\n                end_idx = (head + 1) * head_dim\n                \n                if head not in heads_to_prune:\n                    keep_indices += list(range(start_idx, end_idx))\n            \n            # Convert the keep indices into a LongTensor\n            keep_indices_tensor = torch.LongTensor(keep_indices)\n\n            model.model.layers[block_num].self_attn.q_proj = prune_linear_layer(model.model.layers[block_num].self_attn.q_proj, keep_indices_tensor)\n            model.model.layers[block_num].self_attn.k_proj = prune_linear_layer(model.model.layers[block_num].self_attn.k_proj, keep_indices_tensor)\n            model.model.layers[block_num].self_attn.v_proj = prune_linear_layer(model.model.layers[block_num].self_attn.v_proj, keep_indices_tensor)\n            model.model.layers[block_num].self_attn.o_proj = prune_linear_layer(model.model.layers[block_num].self_attn.o_proj, keep_indices_tensor)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T05:53:45.484191Z","iopub.execute_input":"2024-10-13T05:53:45.485041Z","iopub.status.idle":"2024-10-13T05:53:45.494504Z","shell.execute_reply.started":"2024-10-13T05:53:45.485002Z","shell.execute_reply":"2024-10-13T05:53:45.493568Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def evaluate_fitness(chromosome, dataset):\n    del model\n    \n    df_100['generated_summary'] = dataset['text'].apply(summarize_text)\n    df_100['reference-summary'] = dataset['reference-summary']\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n    # List to store scores\n    results = []\n    \n    # Iterate through the first 100 rows to compute BLEU and ROUGE scores\n    for i, row in df_100.iterrows():\n        reference = row['reference-summary']\n        hypothesis = row['generated_summary']\n        \n        # Calculate BLEU and ROUGE scores\n        bleu_score, rouge_scores = compute_scores(reference, hypothesis)\n        \n        # Store results\n        results.append({\n            'id': row['id'],\n            'bleu_score': bleu_score,\n            'rouge1': rouge_scores['rouge1'].fmeasure,\n            'rouge2': rouge_scores['rouge2'].fmeasure,\n            'rougeL': rouge_scores['rougeL'].fmeasure\n        })\n    \n    # Convert results to DataFrame\n    results_df = pd.DataFrame(results)\n    generated_summaries = df_100['generated_summary'].tolist()\n    reference_summaries = df_100['reference-summary'].tolist()\n\n    # Calculate BERTScore\n    P, R, F1 = score(generated_summaries, reference_summaries, lang='en', verbose=True)\n    return P.mean().item()\n\n\ndef compute_scores(reference, hypothesis):\n    # BLEU score\n    bleu_score = sentence_bleu([reference.split()], hypothesis.split())\n    \n    # ROUGE score\n    rouge_scores = scorer.score(reference, hypothesis)\n    \n    return bleu_score, rouge_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T05:52:21.665062Z","iopub.execute_input":"2024-10-13T05:52:21.665615Z","iopub.status.idle":"2024-10-13T05:52:21.757685Z","shell.execute_reply.started":"2024-10-13T05:52:21.665559Z","shell.execute_reply":"2024-10-13T05:52:21.756607Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"SPARSITY_RATE = 0.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T05:52:21.759347Z","iopub.execute_input":"2024-10-13T05:52:21.760357Z","iopub.status.idle":"2024-10-13T05:52:21.885812Z","shell.execute_reply.started":"2024-10-13T05:52:21.760290Z","shell.execute_reply":"2024-10-13T05:52:21.884709Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def find_size(model):\n  total_size_in_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n  total_size_in_megabytes = total_size_in_bytes / (1024 ** 2)\n  print(f\"Model size: {total_size_in_megabytes:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T05:52:21.887203Z","iopub.execute_input":"2024-10-13T05:52:21.887617Z","iopub.status.idle":"2024-10-13T05:52:21.968374Z","shell.execute_reply.started":"2024-10-13T05:52:21.887556Z","shell.execute_reply":"2024-10-13T05:52:21.967150Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"del model\nmodel = make_model()\nfind_size(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T05:52:21.969980Z","iopub.execute_input":"2024-10-13T05:52:21.970873Z","iopub.status.idle":"2024-10-13T05:52:26.515622Z","shell.execute_reply.started":"2024-10-13T05:52:21.970835Z","shell.execute_reply":"2024-10-13T05:52:26.514696Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afabe5fe7b9e4c0b870a4e40ec1489cc"}},"metadata":{}},{"name":"stdout","text":"Model size: 9560.29 MB\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"model.config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T05:53:31.616213Z","iopub.execute_input":"2024-10-13T05:53:31.616612Z","iopub.status.idle":"2024-10-13T05:53:31.623844Z","shell.execute_reply.started":"2024-10-13T05:53:31.616566Z","shell.execute_reply":"2024-10-13T05:53:31.622925Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"GemmaConfig {\n  \"_name_or_path\": \"/kaggle/input/gemma/transformers/2b/2\",\n  \"architectures\": [\n    \"GemmaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"head_dim\": 256,\n  \"hidden_act\": \"gelu\",\n  \"hidden_activation\": \"gelu_pytorch_tanh\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 16384,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"gemma\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 18,\n  \"num_key_value_heads\": 1,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.44.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 256000\n}"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"chrom = initialize_chromosome(model.config.num_attention_heads*model.config.num_hidden_layers)\nmodel = modify_model(model, chrom)\nfind_size(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T05:53:50.493305Z","iopub.execute_input":"2024-10-13T05:53:50.494009Z","iopub.status.idle":"2024-10-13T05:53:50.531317Z","shell.execute_reply.started":"2024-10-13T05:53:50.493970Z","shell.execute_reply":"2024-10-13T05:53:50.530400Z"}},"outputs":[{"name":"stdout","text":"Model size: 8948.29 MB\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"evalua","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}