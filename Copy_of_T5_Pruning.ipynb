{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet datasets\n",
        "!pip install --quiet evaluate\n",
        "!pip install --quiet accelerate\n",
        "!pip install --quiet bitsandbytes"
      ],
      "metadata": {
        "id": "1ryZjCbQeunj",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C57vO77Ra9B1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "65af88c6-2af4-4c32-90c4-c500b2f803cf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'pyarrow.lib' has no attribute 'ListViewType'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-82359b32a93f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluation_suite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluationSuite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m from .evaluator import (\n\u001b[1;32m     31\u001b[0m     \u001b[0mAudioClassificationEvaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/evaluation_suite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"3.0.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_reader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthread_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     raise ImportError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/_parquet.pyx\u001b[0m in \u001b[0;36minit pyarrow._parquet\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow.lib' has no attribute 'ListViewType'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import evaluate\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "import accelerate\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"SamLowe/roberta-base-go_emotions\""
      ],
      "metadata": {
        "id": "Vcfl2feUdzWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model():\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_name, trust_remote_code=True,\n",
        "                                                           device_map=device)\n",
        "  return model"
      ],
      "metadata": {
        "id": "VjpWr65qszUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pretrained model and tokenizer\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True, # enable 4-bit quantization\n",
        "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
        "    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
        "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
        ")\n",
        "model = make_model()\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")"
      ],
      "metadata": {
        "id": "DDQmZmXobbib",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "j9tBvOzxlSYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_structure(model):\n",
        "    # Number of layers and heads from the config\n",
        "    num_layers = model.config.num_hidden_layers\n",
        "    num_heads = model.config.num_attention_heads\n",
        "    print(f\"Number of layers: {num_layers}\")\n",
        "    print(f\"Number of attention heads per layer: {num_heads}\")\n",
        "\n",
        "    # Check the encoder structure\n",
        "    num_layers_in_encoder = len(model.roberta.encoder.layer)\n",
        "    print(f\"Number of layers in encoder: {num_layers_in_encoder}\")\n",
        "\n",
        "    # Loop through each layer and print the number of heads\n",
        "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
        "        print(f\"Layer {i+1}: {num_heads} attention heads\")\n",
        "\n",
        "# Example usage with a loaded model\n",
        "# Make sure to replace 'model' with your actual model name if it's different\n",
        "print_model_structure(model)\n"
      ],
      "metadata": {
        "id": "HM57PtUUB4IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "split_name = \"validation\"\n",
        "\n",
        "dataset_name, dataset_config_name = \"go_emotions\", \"simplified\"\n",
        "dataset_dict = datasets.load_dataset(dataset_name, dataset_config_name)\n",
        "dataset_dict[split_name][0]"
      ],
      "metadata": {
        "id": "ICphpP9abfxc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset_dict))\n",
        "dataset_dict['validation'][0:10]"
      ],
      "metadata": {
        "id": "rf5FpCkqyaFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def pad_labels(labels, max_length):\n",
        "    \"\"\"Pad the list of labels to a fixed length.\"\"\"\n",
        "    padded_labels = []\n",
        "    for label_list in labels:\n",
        "        # Create a zero-filled tensor with max_length\n",
        "        padded = np.zeros(max_length, dtype=int)\n",
        "        padded[:len(label_list)] = label_list  # Fill in the actual labels\n",
        "        padded_labels.append(padded)\n",
        "    return np.array(padded_labels)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    tokenized_inputs = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "    max_label_length = max(len(label_list) for label_list in examples['labels'])\n",
        "    padded_labels = pad_labels(examples['labels'], max_label_length)\n",
        "\n",
        "    tokenized_inputs['label'] = padded_labels.tolist()  # Convert numpy array to list\n",
        "    return tokenized_inputs\n",
        "\n",
        "encoded_dataset = dataset_dict.map(preprocess_function, batched=True)\n",
        "\n",
        "print(encoded_dataset['validation'][0])"
      ],
      "metadata": {
        "id": "np6xWRrqbjrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    input_ids = pad_sequence([torch.tensor(item['input_ids']) for item in batch], batch_first=True)\n",
        "    attention_mask = pad_sequence([torch.tensor(item['attention_mask']) for item in batch], batch_first=True)\n",
        "    labels = pad_sequence([torch.tensor(item['label']) for item in batch], batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'label': labels\n",
        "    }"
      ],
      "metadata": {
        "id": "zh91jRwh7dVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DataLoader to create batches\n",
        "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "print(encoded_dataset['validation'])"
      ],
      "metadata": {
        "id": "zgnpdil6vmUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify model based on chromosome\n",
        "def modify_model(model, chromosome):\n",
        "    num_heads = model.config.num_attention_heads\n",
        "    num_blocks = model.config.num_hidden_layers\n",
        "    print(num_blocks)\n",
        "    # num_ffn = model.config.num_hidden_layers\n",
        "\n",
        "    # Disable attention heads\n",
        "    heads_to_prune = defaultdict(list)\n",
        "    for i, gene in enumerate(chromosome):\n",
        "        if gene == 0:\n",
        "          block_num = i//num_heads\n",
        "          head_num = i%num_heads\n",
        "          heads_to_prune[block_num].append(head_num)\n",
        "\n",
        "    print(heads_to_prune)\n",
        "    if heads_to_prune:\n",
        "\n",
        "        for block_num in heads_to_prune:\n",
        "            # model.transformer.encoder.block[block_num].layer[0].SelfAttention.prune_heads(heads_to_prune[block_num])\n",
        "            model.roberta.encoder.layer[block_num].attention.prune_heads(heads_to_prune[block_num])\n",
        "            # if(bloack_num!=0):\n",
        "            # model.transformer.decoder.block[block_num].layer[0].SelfAttention.prune_heads(heads_to_prune[block_num])\n",
        "            # model.transformer.decoder.block[block_num].layer[1].EncDecAttention.prune_heads(heads_to_prune[block_num])\n",
        "\n",
        "            # print(block_num, model.transformer.encoder.block[block_num].layer[0].SelfAttention.n_heads)\n",
        "            # model.prune_heads(heads_to_prune)\n",
        "\n",
        "            # model.transformer.encoder.block[block_num].layer[0].SelfAttention.q = prune_linear_layer(model.transformer.encoder.block[block_num].layer[0].SelfAttention.q, index)\n",
        "            # model.transformer.encoder.block[block_num].layer[0].SelfAttention.k = prune_linear_layer(model.transformer.encoder.block[block_num].layer[0].SelfAttention.k, index)\n",
        "            # model.transformer.encoder.block[block_num].layer[0].SelfAttention.v = prune_linear_layer(model.transformer.encoder.block[block_num].layer[0].SelfAttention.v, index)\n",
        "            # model.transformer.encoder.block[block_num].layer[0].SelfAttention.o = prune_linear_layer(model.transformer.encoder.block[block_num].layer[0].SelfAttention.o, index, dim=1)"
      ],
      "metadata": {
        "id": "ZHxSB1I0bn-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import hamming_loss, accuracy_score\n",
        "\n",
        "def evaluate_fitness(chromosome, model, encoded_dataset, metric='accuracy'):\n",
        "    # Modify the model according to the chromosome\n",
        "    if len(chromosome):\n",
        "        model = make_model()\n",
        "        modify_model(model, chromosome)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "\n",
        "    dataloader = DataLoader(encoded_dataset['test'], batch_size=16, collate_fn=collate_fn)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Get predictions\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            labels.extend(batch['label'].cpu().numpy().tolist())  # Convert to list here\n",
        "\n",
        "    # Convert labels to binary format using MultiLabelBinarizer\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    labels_binary = mlb.fit_transform(labels)  # Convert labels into binary format\n",
        "\n",
        "    # Create binary predictions\n",
        "    predictions_binary = np.zeros((len(predictions), len(mlb.classes_)), dtype=int)\n",
        "    for idx, pred in enumerate(predictions):\n",
        "        predictions_binary[idx, pred] = 1  # Set the predicted class to 1\n",
        "\n",
        "    # Calculate Hamming Loss\n",
        "    # fitness = 1 - hamming_loss(labels_binary, predictions_binary)  # Higher is better\n",
        "\n",
        "    accuracy = accuracy_score(labels_binary, predictions_binary)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"For the chromosome\", chromosome)\n",
        "    print(f\"Validation Hamming Loss: {accuracy:.4f}\")\n",
        "    end_time = time.time()\n",
        "    print(\"Execution time = \", end_time - start_time)\n",
        "    print()\n",
        "\n",
        "    return fitness\n"
      ],
      "metadata": {
        "id": "BvaktIYnbqyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# without modifying the model\n",
        "print(evaluate_fitness([], model, encoded_dataset))"
      ],
      "metadata": {
        "id": "S2FwHJjs1ClV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "06bd28b2-c391-4bb2-84af-eb381a5a8769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-893037bd025a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# without modifying the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_fitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "FLFMXsKwJDND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8600ab6-c896-4ef4-8ac8-3829b2a12e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_random_binary_list(length, percentage_of_zeros):\n",
        "    # Calculate the number of zeros and ones based on the percentage\n",
        "    num_zeros = int(length * (percentage_of_zeros / 100))\n",
        "    num_ones = length - num_zeros\n",
        "\n",
        "    # Create the list with the required number of 0s and 1s\n",
        "    binary_list = [0] * num_zeros + [1] * num_ones\n",
        "\n",
        "    # Shuffle the list to randomize the order\n",
        "    random.shuffle(binary_list)\n",
        "\n",
        "    return binary_list\n",
        "\n",
        "def initialize_chromosome(num_genes):\n",
        "  # initialize chromosome with given sparsity percentage\n",
        "  return create_random_binary_list(num_genes, 30)"
      ],
      "metadata": {
        "id": "4mHUj59O30pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking to see if decoder is there\n",
        "print(model.config.is_decoder)"
      ],
      "metadata": {
        "id": "yEoH-2Li5O07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c50791-08cd-40d5-9a4f-6b25a1f078fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model  = make_model()\n",
        "chromosome = initialize_chromosome(model.config.num_attention_heads*model.config.num_hidden_layers)\n",
        "print(\"Chromosome:\", chromosome)\n",
        "fitness = evaluate_fitness(chromosome, model, encoded_dataset)\n",
        "print(\"Fitness score (accuracy):\", fitness)\n"
      ],
      "metadata": {
        "id": "jCNP5DfCbTES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "4213fcde-055c-4da9-ad2f-b2cfde9a92eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chromosome: [0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1]\n",
            "12\n",
            "defaultdict(<class 'list'>, {0: [0, 3, 4, 6, 8], 1: [2, 5, 6], 2: [2, 8], 3: [4, 7], 4: [3, 11], 5: [1, 3, 7, 9, 10, 11], 6: [0, 9, 10, 11], 7: [4, 5, 7, 8], 8: [1, 4, 5, 6, 8], 9: [0, 10], 10: [0, 3, 4, 7, 8, 11], 11: [6, 7]})\n",
            "For the chromosome [0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1]\n",
            "Number of predictions: 5427, Number of labels: 5427\n",
            "Validation Hamming Loss: 0.9353\n",
            "Execution time =  32.64664602279663\n",
            "\n",
            "Fitness score (accuracy): 0.9352575745610572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_size(model):\n",
        "  total_size_in_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "  total_size_in_megabytes = total_size_in_bytes / (1024 ** 2)\n",
        "  print(f\"Model size: {total_size_in_megabytes:.2f} MB\")\n",
        "\n",
        "find_size(model)"
      ],
      "metadata": {
        "id": "I4_Q6d-62AhC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f697083-4402-4452-f14e-4f956d845b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 475.57 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# m  = AutoModelForSequenceClassification.from_pretrained(model_name, trust_remote_code=True, device_map=device)\n",
        "# print(find_size(m), find_size(model))"
      ],
      "metadata": {
        "id": "u43nPciC2aIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "POPN_SIZE = 10\n",
        "crossover_rate = 0.7\n",
        "mutation_rate = 0.1"
      ],
      "metadata": {
        "id": "mS6i_s8nLDdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_population(chromosome_length):\n",
        "  # initialize random population\n",
        "  population = []\n",
        "  for _ in range(POPN_SIZE):\n",
        "    chromosome = initialize_chromosome(chromosome_length)\n",
        "    population.append(chromosome)\n",
        "  return population"
      ],
      "metadata": {
        "id": "L6b8oDNuIVE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selection (Tournament Selection)\n",
        "'''\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(POPN_SIZE):\n",
        "        tournament = np.random.choice(POPN_SIZE, 2)\n",
        "        print(\"tournament = \",tournament)\n",
        "        winner = tournament[np.argmax(fitness_scores[tournament])]\n",
        "        parents.append(population[winner])\n",
        "    return np.array(parents)\n",
        "    '''"
      ],
      "metadata": {
        "id": "g6JzUlKUk4Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# roulette wheel selection\n",
        "def select_parents(population, fitness_scores, num_parents):\n",
        "    # Normalize fitness scores to create a probability distribution\n",
        "    total_fitness = np.sum(fitness_scores)\n",
        "    probabilities = fitness_scores / total_fitness\n",
        "\n",
        "    # Select parents based on their fitness proportion (roulette wheel selection)\n",
        "    # selected_parents = np.random.choice(population, size=num_parents, p=probabilities, replace=True)\n",
        "    selected_parents = random.choices(population, weights=probabilities, k=num_parents)\n",
        "\n",
        "\n",
        "    return np.array(selected_parents)"
      ],
      "metadata": {
        "id": "Mr_ly1rXhQqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crossover (Single-point crossover)\n",
        "def crossover(parent1, parent2):\n",
        "    if np.random.rand() < crossover_rate:\n",
        "        point = np.random.randint(1, len(parent1) - 1)\n",
        "        child1 = np.concatenate([parent1[:point], parent2[point:]])\n",
        "        child2 = np.concatenate([parent2[:point], parent1[point:]])\n",
        "    else:\n",
        "        child1, child2 = parent1, parent2\n",
        "    return child1, child2"
      ],
      "metadata": {
        "id": "0X9ruDzQk5ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mutation (Flip bit mutation)\n",
        "def mutate(chromosome):\n",
        "    for i in range(len(chromosome)):\n",
        "        if np.random.rand() < mutation_rate:\n",
        "            chromosome[i] = 1 - chromosome[i]\n",
        "    return chromosome"
      ],
      "metadata": {
        "id": "hPGjVtb2k_L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def elitism_and_selection(population, fitness_scores, num_elites, num_parents):\n",
        "    # Elitism: Keep the top num_elites individuals\n",
        "    elite_indices = np.argsort(fitness_scores)[-num_elites:]  # Get indices of top individuals\n",
        "    elites = [population[i] for i in elite_indices]\n",
        "\n",
        "    # Perform roulette wheel selection for the rest of the parents\n",
        "    remaining_population = np.delete(population, elite_indices, axis=0)\n",
        "    remaining_fitness_scores = np.delete(fitness_scores, elite_indices)\n",
        "\n",
        "    num_to_select = num_parents - num_elites\n",
        "    selected_parents = select_parents(remaining_population, remaining_fitness_scores, num_to_select)\n",
        "\n",
        "    # Combine elites and selected parents\n",
        "    next_generation = np.vstack((elites, selected_parents))\n",
        "\n",
        "    return next_generation"
      ],
      "metadata": {
        "id": "onHVyxjsxNIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def genetic_algorithm(model, num_generations, desired_sparsity):\n",
        "  population = initialize_population(model.config.num_heads*model.config.num_layers)\n",
        "  for generation in range(num_generations):\n",
        "      fitness_scores = np.array([evaluate_fitness(chrom, model, encoded_dataset, metric) for chrom in population])\n",
        "      parents = select_parents(population, fitness_scores)\n",
        "      new_population = []\n",
        "      for i in range(0, POPN_SIZE, 2):\n",
        "          parent1, parent2 = parents[i], parents[i + 1]\n",
        "          child1, child2 = crossover(parent1, parent2)\n",
        "          child1 = mutate(child1)\n",
        "          child2 = mutate(child2)\n",
        "          new_population.extend([child1, child2])\n",
        "      population = np.array(new_population)\n",
        "\n",
        "      # Check for desired sparsity level\n",
        "      sparsity_levels = np.mean(population == 0, axis=1)\n",
        "      print(sparsity_levels)\n",
        "      if np.any(sparsity_levels >= desired_sparsity):\n",
        "          best_chromosome = population[np.argmax(sparsity_levels)]\n",
        "          break\n",
        "  return best_chromosome'''"
      ],
      "metadata": {
        "id": "Kajvda-3lD5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genetic_algorithm(model, num_generations, desired_sparsity):\n",
        "  population = initialize_population(model.config.num_attention_heads*model.config.num_hidden_layers)  # Initialize the population\n",
        "  for generation in range(num_generations):\n",
        "      fitness_scores = np.array([evaluate_fitness(chrom, model, encoded_dataset) for chrom in population])\n",
        "      best_chromosome = population[np.argmax(fitness_scores)]\n",
        "      print(\"new fitness scores:\", fitness_scores)\n",
        "      print(f\"best chromosome in generation {generation} is {best_chromosome} with accuracy {fitness_scores[np.argmax(fitness_scores)]}\")\n",
        "      parents = elitism_and_selection(population, fitness_scores, 4, POPN_SIZE)\n",
        "      # parents = select_parents(population, fitness_scores, POPN_SIZE)\n",
        "      new_population = []\n",
        "      for i in range(0, POPN_SIZE, 2):\n",
        "          parent1, parent2 = parents[i], parents[i + 1]\n",
        "          child1, child2 = crossover(parent1, parent2)\n",
        "          child1 = mutate(child1)\n",
        "          child2 = mutate(child2)\n",
        "          new_population.extend([child1, child2])\n",
        "      population = np.array(new_population)\n",
        "  return best_chromosome"
      ],
      "metadata": {
        "id": "oHyz4GxUxRax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''genetic_algorithm(model, 10, 0.3)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Btaww4af6KNz",
        "outputId": "fb669411-49c7-4b2c-94fe-72e609db89b4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [0, 1, 4], 1: [2, 5, 6, 7], 2: [1], 3: [4, 6], 4: [1, 7], 5: [1, 7]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]\n",
            "Validation Accuracy: 0.3538\n",
            "Execution time =  220.13971972465515\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [5], 1: [3, 4, 5, 6], 2: [4, 7], 4: [2, 3, 4, 5], 5: [1, 5, 7]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0]\n",
            "Validation Accuracy: 0.3730\n",
            "Execution time =  222.45740389823914\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [0], 1: [1, 5], 2: [3, 6], 3: [3, 4, 7], 4: [1, 3, 6], 5: [2, 6, 7]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0]\n",
            "Validation Accuracy: 0.5360\n",
            "Execution time =  219.88337445259094\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [1, 3, 5], 1: [2, 4], 2: [1], 3: [1, 3, 7], 4: [0, 2, 4, 6], 5: [4]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
            "Validation Accuracy: 0.6635\n",
            "Execution time =  209.18546509742737\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [1], 1: [0, 5, 7], 2: [0, 2, 4], 3: [3], 4: [1, 5], 5: [0, 1, 2, 3]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1]\n",
            "Validation Accuracy: 0.6625\n",
            "Execution time =  212.0702304840088\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [0, 2, 3], 1: [1, 3, 5, 6], 3: [7], 4: [0, 2], 5: [0, 3, 4, 5]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1]\n",
            "Validation Accuracy: 0.6088\n",
            "Execution time =  210.9596984386444\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [0, 4, 6], 1: [4, 5], 2: [6], 3: [0, 1, 3, 4], 4: [0, 1], 5: [0, 6]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1]\n",
            "Validation Accuracy: 0.3873\n",
            "Execution time =  208.51832008361816\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [7], 1: [0, 4, 6, 7], 2: [0, 2, 4, 6], 4: [2], 5: [2, 3, 4, 5]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n",
            "Validation Accuracy: 0.6826\n",
            "Execution time =  209.6526551246643\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [0, 3, 4, 5], 1: [3, 4], 2: [4, 5, 7], 4: [1, 3, 4, 7], 5: [0]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
            "Validation Accuracy: 0.3087\n",
            "Execution time =  211.3256130218506\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [2, 4, 5], 1: [2, 5], 2: [3, 4, 6, 7], 3: [3, 4], 4: [3, 4], 5: [5]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
            "Validation Accuracy: 0.4497\n",
            "Execution time =  210.25780868530273\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [6], 1: [1, 4, 5, 7], 2: [0, 5], 3: [1, 3], 4: [0, 6, 7], 5: [2, 4]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1]\n",
            "Validation Accuracy: 0.3452\n",
            "Execution time =  209.0882956981659\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [3, 4, 7], 2: [1, 4, 6, 7], 3: [5, 6], 4: [3, 6], 5: [0, 3, 6]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n",
            "Validation Accuracy: 0.3423\n",
            "Execution time =  213.78472328186035\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [2, 5, 7], 1: [1, 2, 3, 4, 6, 7], 2: [2, 7], 3: [0, 3, 7]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Validation Accuracy: 0.6826\n",
            "Execution time =  210.889000415802\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [5], 1: [2, 6, 7], 2: [2, 4], 3: [4], 4: [0, 1, 2, 6], 5: [0, 1, 3]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
            "Validation Accuracy: 0.3087\n",
            "Execution time =  209.69819355010986\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [5, 7], 1: [1, 4], 2: [0, 6], 3: [0, 6, 7], 4: [1, 6], 5: [2, 4, 7]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0]\n",
            "Validation Accuracy: 0.6913\n",
            "Execution time =  209.63826298713684\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [0, 2, 4, 6, 7], 1: [4], 2: [0, 5], 3: [0, 2, 3], 4: [0, 7], 5: [5]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1]\n",
            "Validation Accuracy: 0.3087\n",
            "Execution time =  211.9301302433014\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [4, 5], 1: [2], 2: [0, 1, 2, 7], 3: [1, 2, 3, 5], 4: [5], 5: [3, 6]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1]\n",
            "Validation Accuracy: 0.6779\n",
            "Execution time =  211.87941765785217\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [1, 3], 1: [0, 3], 2: [5], 3: [2, 3, 4], 4: [0, 3, 4, 6], 5: [3, 7]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0]\n",
            "Validation Accuracy: 0.6913\n",
            "Execution time =  210.9225471019745\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [5, 6], 1: [3, 4], 2: [3, 5, 6], 3: [0, 5], 4: [1, 6, 7], 5: [3, 7]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0]\n",
            "Validation Accuracy: 0.3087\n",
            "Execution time =  211.41342115402222\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "defaultdict(<class 'list'>, {0: [1, 7], 1: [7], 2: [2, 3, 7], 3: [1, 3, 5, 6], 4: [4], 5: [2, 3, 6]})\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "Pruning\n",
            "For the chromosome [1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1]\n",
            "Validation Accuracy: 0.6913\n",
            "Execution time =  212.1215159893036\n",
            "\n",
            "tournament =  [5 5]\n",
            "tournament =  [ 1 19]\n",
            "tournament =  [13 19]\n",
            "tournament =  [2 6]\n",
            "tournament =  [8 7]\n",
            "tournament =  [18 11]\n",
            "tournament =  [16 16]\n",
            "tournament =  [12  4]\n",
            "tournament =  [10 15]\n",
            "tournament =  [3 1]\n",
            "tournament =  [2 5]\n",
            "tournament =  [6 8]\n",
            "tournament =  [14 14]\n",
            "tournament =  [6 5]\n",
            "tournament =  [ 2 15]\n",
            "tournament =  [ 4 17]\n",
            "tournament =  [15  7]\n",
            "tournament =  [12 16]\n",
            "tournament =  [15  5]\n",
            "tournament =  [ 3 12]\n",
            "[0.35416667 0.3125     0.33333333 0.22916667 0.33333333 0.41666667\n",
            " 0.3125     0.39583333 0.25       0.35416667 0.3125     0.33333333\n",
            " 0.35416667 0.33333333 0.375      0.41666667 0.29166667 0.27083333\n",
            " 0.3125     0.39583333]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
              "       0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# genetic_algorithm(model, num_generations, desired_sparsity)\n",
        "genetic_algorithm(model, 2, 0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewkSsqDWx19C",
        "outputId": "82c91b30-592e-4004-f5a7-eca21abe7638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "defaultdict(<class 'list'>, {0: [0, 1, 3, 4, 5, 7, 9, 11], 1: [1, 8, 9, 11], 2: [0, 2, 3, 5, 7, 9], 4: [0, 5], 5: [1, 2, 3, 6, 7, 8, 9], 6: [1, 2, 5, 10], 7: [4, 6, 9], 8: [1, 8, 9], 9: [1], 10: [0, 8], 11: [3, 7, 8]})\n",
            "For the chromosome [0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1]\n",
            "Number of predictions: 5427, Number of labels: 5427\n",
            "Validation Hamming Loss: 0.9358\n",
            "Execution time =  34.42866921424866\n",
            "\n",
            "12\n",
            "defaultdict(<class 'list'>, {0: [4], 1: [4, 6, 10, 11], 2: [0, 1, 3, 7, 9], 3: [0, 3, 4], 4: [0, 5, 6, 7], 5: [2, 6, 7, 8, 11], 6: [0, 1, 4, 10], 7: [3, 4, 5, 8, 11], 8: [2, 3, 6], 9: [1, 3, 11], 10: [2, 7, 9], 11: [0, 6, 7]})\n",
            "For the chromosome [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1]\n",
            "Number of predictions: 5427, Number of labels: 5427\n",
            "Validation Hamming Loss: 0.9354\n",
            "Execution time =  33.61736845970154\n",
            "\n",
            "12\n",
            "defaultdict(<class 'list'>, {0: [1, 3, 4], 1: [4, 5, 6, 11], 2: [5, 6, 7, 8], 3: [0, 1, 5, 6, 10], 4: [7, 10], 5: [0, 8, 10, 11], 6: [4, 10, 11], 7: [2, 3, 5, 9, 10], 8: [5, 6, 7], 9: [0, 3, 7, 10], 10: [3, 7, 11], 11: [0, 2, 3]})\n",
            "For the chromosome [1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Number of predictions: 5427, Number of labels: 5427\n",
            "Validation Hamming Loss: 0.9344\n",
            "Execution time =  32.75185751914978\n",
            "\n",
            "12\n",
            "defaultdict(<class 'list'>, {0: [0, 2, 4, 5, 8], 1: [2, 5, 6, 7, 11], 2: [4, 5, 8], 3: [1, 3, 4, 6, 11], 4: [4], 5: [1, 3, 9], 6: [8, 9, 11], 7: [2, 4, 5, 6, 7, 11], 8: [1, 2, 8], 9: [1, 3], 10: [0, 6, 7, 10], 11: [0, 4, 10]})\n",
            "For the chromosome [0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1]\n",
            "Number of predictions: 5427, Number of labels: 5427\n",
            "Validation Hamming Loss: 0.9365\n",
            "Execution time =  33.601171255111694\n",
            "\n",
            "12\n",
            "defaultdict(<class 'list'>, {0: [0, 6, 9, 11], 1: [0, 2, 3, 6], 2: [1, 7], 3: [3, 7, 8, 9, 11], 4: [0, 1, 5, 6, 7, 8, 10], 5: [2], 6: [0, 3, 10], 7: [1, 8], 8: [0, 8, 9, 11], 9: [3, 7, 8], 10: [0, 1, 3, 6, 8, 11], 11: [2, 9]})\n",
            "For the chromosome [0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
            "Number of predictions: 5427, Number of labels: 5427\n",
            "Validation Hamming Loss: 0.9375\n",
            "Execution time =  33.21594429016113\n",
            "\n",
            "12\n",
            "defaultdict(<class 'list'>, {0: [1, 8, 9], 1: [2, 3, 9, 11], 2: [0, 3], 3: [1, 5, 7, 8], 4: [2], 5: [3, 6, 8, 9, 10, 11], 6: [5], 7: [0, 1, 3, 4, 6, 10], 8: [1, 2, 3, 6, 7, 8, 10], 9: [0, 3, 5, 8], 10: [5, 6, 7], 11: [4, 8]})\n",
            "For the chromosome [1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1]\n",
            "Number of predictions: 5427, Number of labels: 5427\n",
            "Validation Hamming Loss: 0.9367\n",
            "Execution time =  33.1538872718811\n",
            "\n",
            "12\n",
            "defaultdict(<class 'list'>, {0: [3, 10, 11], 1: [5, 6], 2: [3, 5, 6], 3: [3, 7, 8, 9], 4: [1, 5, 6, 7], 5: [0, 3, 5, 6], 6: [0, 11], 7: [0, 4, 5, 6, 7, 8, 10], 8: [5], 9: [0, 1, 2, 3, 4, 6, 9], 10: [7, 9], 11: [2, 5, 10, 11]})\n",
            "For the chromosome [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
            "Number of predictions: 5427, Number of labels: 5427\n",
            "Validation Hamming Loss: 0.9355\n",
            "Execution time =  33.53137707710266\n",
            "\n",
            "12\n",
            "defaultdict(<class 'list'>, {0: [1, 7, 9], 1: [1, 2, 4, 6, 9, 11], 2: [1, 2, 10], 3: [0, 4, 10], 4: [4, 9, 10, 11], 5: [4, 5, 7, 8], 6: [2, 5, 6, 10], 7: [3, 5, 11], 8: [0, 2, 8, 11], 9: [0, 1, 9], 10: [4, 6, 7], 11: [2, 3, 5]})\n",
            "For the chromosome [1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1]\n",
            "Number of predictions: 5427, Number of labels: 5427\n",
            "Validation Hamming Loss: 0.9369\n",
            "Execution time =  33.25573444366455\n",
            "\n",
            "12\n",
            "defaultdict(<class 'list'>, {0: [3, 4, 8], 1: [1, 3, 5, 6, 8], 2: [2, 5, 8, 11], 3: [2, 4], 4: [1, 11], 5: [2, 3, 9, 10], 7: [0, 5, 8], 8: [4, 9, 11], 9: [3, 6, 7, 10], 10: [1, 2, 5, 7, 9, 11], 11: [0, 1, 3, 5, 6, 7, 8]})\n",
            "For the chromosome [1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1]\n",
            "Number of predictions: 5427, Number of labels: 5427\n",
            "Validation Hamming Loss: 0.9345\n",
            "Execution time =  33.72654461860657\n",
            "\n",
            "12\n",
            "defaultdict(<class 'list'>, {0: [3, 6, 10], 1: [0], 2: [4, 8], 3: [1, 3, 4, 6, 7, 10], 4: [3, 10, 11], 5: [2, 3, 6, 7, 9, 10, 11], 6: [3, 5, 10], 7: [1, 7, 9], 8: [0, 4, 5, 9], 9: [8, 11], 10: [0, 4, 7, 10], 11: [4, 5, 6, 9, 11]})\n",
            "For the chromosome [1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0]\n",
            "Number of predictions: 5427, Number of labels: 5427\n",
            "Validation Hamming Loss: 0.9345\n",
            "Execution time =  33.416353940963745\n",
            "\n",
            "new fitness scores: [0.93578404 0.93538919 0.93436258 0.93649477 0.93753455 0.93666588\n",
            " 0.93552081 0.93687646 0.93446787 0.93454684]\n",
            "best chromosome in generation 0 is [0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1] with accuracy 0.9375345494748479\n",
            "12\n",
            "defaultdict(<class 'list'>, {0: [0, 2, 4, 5, 6, 8, 11], 1: [1, 2, 5, 6, 7, 8, 11], 2: [4, 5, 8], 3: [1, 2, 3, 4, 6], 4: [1, 6], 5: [1, 3, 9], 6: [8, 9], 7: [2, 4, 5, 6, 7, 11], 8: [1, 2, 8], 9: [1, 3, 5], 10: [1, 5, 6, 7, 9], 11: [3, 4, 8]})\n"
          ]
        }
      ]
    }
  ]
}